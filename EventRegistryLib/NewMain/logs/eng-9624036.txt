{"uri": "eng-9624036", "concepts": [{"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 100, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 96, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 50, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 43, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 39, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 32, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 31, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 29, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 27, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 25, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 24, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 24, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 23, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 19, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 19, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 18, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 18, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 17, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 16, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 14, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 14, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Arms_race", "type": "wiki", "score": 13, "label": {"eng": "Arms race"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 13, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/CNBC", "type": "org", "score": 10, "label": {"eng": "CNBC"}}, {"uri": "http://en.wikipedia.org/wiki/Meta_Platforms", "type": "org", "score": 9, "label": {"eng": "Meta Platforms"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 9, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 9, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 8, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 8, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Vox_(website)", "type": "wiki", "score": 6, "label": {"eng": "Vox (website)"}}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 5, "label": {"eng": "Scarlett Johansson"}}], "eventDate": "2024-06-04", "totalArticleCount": 43, "title": {"eng": "OpenAI, Google DeepMind's current and former employees warn about AI risks"}, "summary": {"eng": "June 4 (Reuters) - A group of current and former employees at artificial intelligence (AI) companies, including Microsoft-backed (MSFT.O)New Tab, opens new tab OpenAI and Alphabet's (GOOGL.O)New Tab, opens new tab Google DeepMind on Tuesday raised concerns about risks posed by the emerging technology.\n\nAn open letter by a group of 11 current and former employees of OpenAI and one current and another former employee with Google DeepMind said the financial motives of AI companies hinder effective o"}, "location": null, "categories": [{"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 23}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 24}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 64}], "articleCounts": {"eng": 43}, "sentiment": 0.07450980392156858, "breakingScore": 0.600056006737257}
{"uri": "8161341966", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:03:09", "dateTime": "2024-06-04T15:03:09Z", "dateTimePub": "2024-06-04T15:02:32Z", "dataType": "news", "sim": 0.9019607901573181, "url": "https://www.nbcmiami.com/news/business/money-report/current-and-former-openai-employees-warn-of-ais-serious-risk-and-lack-of-oversight/3327971/", "title": "Current and former OpenAI employees warn of AI's 'serious risk' and lack of oversight", "body": "OpenAI, Google, Microsoft and other companies are at the helm of a generative AI arms race -- a market that is predicted to top $1 trillion in revenue within a decade.\n\nA group of current and former OpenAI employees published an open letter Tuesday describing concerns about the artificial intelligence industry's rapid advancement despite a lack of oversight and an absence of whistleblower protections for those who wish to speak up.\n\n\"AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this,\" the employees wrote in the open letter.\n\nOpenAI, Google, Microsoft, Meta and other companies are at the helm of a generative AI arms race -- a market that is predicted to top $1 trillion in revenue within a decade -- as companies in seemingly every industry rush to add AI-powered chatbots and agents to avoid being left behind by competitors.\n\nThe current and former employees wrote AI companies have \"substantial non-public information\" about what their technology can do, the extent of the safety measures they've put in place and the risk levels that technology has for different types of harm.\n\n\"We also understand the serious risks posed by these technologies,\" they wrote, adding that the companies \"currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThe letter also details the current and former employees' concerns about insufficient whistleblower protections for the AI industry, stating that without effective government oversight, employees are in a relatively unique position to hold companies accountable.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the signatories wrote. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated.\"\n\nThe letter asks AI companies to commit to not entering or enforcing non-disparagement agreements; to create anonymous processes for current and former employees to voice concerns to a company's board, regulators and others; to support a culture of open criticism; and to not retaliate against public whistleblowing if internal reporting processes fail.\n\nFour anonymous OpenAI employees and seven former ones, including Daniel Kokotajlo, Jacob Hilton, William Saunders, Carroll Wainwright and Daniel Ziegler, signed the letter. Signatories also included Ramana Kumar, who formerly worked at Google DeepMind, and Neel Nanda, who currently works at Google DeepMind and formerly worked at Anthropic. Three famed computer scientists known for advancing the artificial intelligence field also endorsed the letter: Geoffrey Hinton, Yoshua Bengio and Stuart Russell.\n\n\"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world,\" an OpenAI spokesperson told CNBC, adding that the company has an anonymous integrity hotline, as well as a Safety and Security Committee led by members of the board and OpenAI leaders.\n\nMicrosoft declined to comment.\n\nLast month, OpenAI backtracked on a controversial decision to make former employees choose between signing a non-disparagement agreement that would never expire, or keeping their vested equity in the company. The internal memo, viewed by CNBC, was sent to former employees and shared with current ones.\n\nThe memo, addressed to each former employee, said that at the time of the person's departure from OpenAI, \"you may have been informed that you were required to execute a general release agreement that included a non-disparagement provision in order to retain the Vested Units [of equity].\"\n\n\"We're incredibly sorry that we're only changing this language now; it doesn't reflect our values or the company we want to be,\" an OpenAI spokesperson told CNBC at the time.\n\nTuesday's open letter also follows OpenAI's decision last month to disband its team focused on the long-term risks of AI just one year after the Microsoft-backed startup announced the group, a person familiar with the situation confirmed to CNBC at the time.\n\nThe person, who spoke on condition of anonymity, said some of the team members are being reassigned to multiple other teams within the company.\n\nThe team's disbandment followed team leaders, OpenAI co-founder Ilya Sutskever and Jan Leike, announcing their departures from the startup last month. Leike wrote in a post on X that OpenAI's \"safety culture and processes have taken a backseat to shiny products.\"\n\nCEO Sam Altman said on X he was sad to see Leike leave and that the company had more work to do. Soon after, OpenAI co-founder Greg Brockman posted a statement attributed to himself and Altman on X, asserting that the company has \"raised awareness of the risks and opportunities of AGI so that the world can better prepare for it.\"\n\n\"I joined because I thought OpenAI would be the best place in the world to do this research,\" Leike wrote on X. \"However, I have been disagreeing with OpenAI leadership about the company's core priorities for quite some time, until we finally reached a breaking point.\"\n\nLeike wrote he believes much more of the company's bandwidth should be focused on security, monitoring, preparedness, safety and societal impact.\n\n\"These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there,\" he wrote. \"Over the past few months my team has been sailing against the wind. Sometimes we were struggling for [computing resources] and it was getting harder and harder to get this crucial research done.\"\n\nLeike added that OpenAI must become a \"safety-first AGI company.\"\n\n\"Building smarter-than-human machines is an inherently dangerous endeavor,\" he wrote. \"OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.\"\n\nThe high-profile departures come months after OpenAI went through a leadership crisis involving Altman.\n\nIn November, OpenAI's board ousted Altman, saying in a statement that Altman had not been \"consistently candid in his communications with the board.\"\n\nThe issue seemed to grow more complex each day, with The Wall Street Journal and other media outlets reporting that Sutskever trained his focus on ensuring that artificial intelligence would not harm humans, while others, including Altman, were instead more eager to push ahead with delivering new technology.\n\nAltman's ouster prompted resignations or threats of resignations, including an open letter signed by virtually all of OpenAI's employees, and uproar from investors, including Microsoft. Within a week, Altman was back at the company, and board members Helen Toner, Tasha McCauley and Ilya Sutskever, who had voted to oust Altman, were out. Sutskever stayed on staff at the time but no longer in his capacity as a board member. Adam D'Angelo, who had also voted to oust Altman, remained on the board.\n\nMeanwhile, last month, OpenAI launched a new AI model and desktop version of ChatGPT, along with an updated user interface and audio capabilities, the company's latest effort to expand the use of its popular chatbot. One week after OpenAI debuted the range of audio voices, the company announced it would pull one of the viral chatbot's voices named \"Sky.\"\n\n\"Sky\" created controversy for resembling the voice of actress Scarlett Johansson in \"Her,\" a movie about artificial intelligence. The Hollywood star has alleged that OpenAI ripped off her voice even though she declined to let them use it.", "source": {"uri": "nbcmiami.com", "dataType": "news", "title": "NBC 6 South Florida"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Arms_race", "type": "wiki", "score": 5, "label": {"eng": "Arms race"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 5, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 5, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 5, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/CNBC", "type": "org", "score": 4, "label": {"eng": "CNBC"}}, {"uri": "http://en.wikipedia.org/wiki/Meta_Platforms", "type": "org", "score": 3, "label": {"eng": "Meta Platforms"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 3, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 2, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 2, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Startup_company", "type": "wiki", "score": 2, "label": {"eng": "Startup company"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 1, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Adam_D'Angelo", "type": "person", "score": 1, "label": {"eng": "Adam D'Angelo"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Bandwidth_(computing)", "type": "wiki", "score": 1, "label": {"eng": "Bandwidth (computing)"}}, {"uri": "http://en.wikipedia.org/wiki/Desktop_computer", "type": "wiki", "score": 1, "label": {"eng": "Desktop computer"}}, {"uri": "http://en.wikipedia.org/wiki/Computing", "type": "wiki", "score": 1, "label": {"eng": "Computing"}}, {"uri": "http://en.wikipedia.org/wiki/Viral_phenomenon", "type": "wiki", "score": 1, "label": {"eng": "Viral phenomenon"}}, {"uri": "http://en.wikipedia.org/wiki/User_interface", "type": "wiki", "score": 1, "label": {"eng": "User interface"}}, {"uri": "http://en.wikipedia.org/wiki/The_Wall_Street_Journal", "type": "wiki", "score": 1, "label": {"eng": "The Wall Street Journal"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 18}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 20}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 21}, {"uri": "dmoz/Business/Human_Resources/Training_and_Safety", "label": "dmoz/Business/Human Resources/Training and Safety", "wgt": 16}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 16}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 64}], "image": "https://media.nbcmiami.com/2024/06/107418094-1716321352537-gettyimages-2153474140-AFP_34TH9TC.jpeg?quality=85&strip=all&resize=1200%2C675", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.1686274509803922, "wgt": 230, "relevance": 1}
{"uri": "8161326315", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "14:52:50", "dateTime": "2024-06-04T14:52:50Z", "dateTimePub": "2024-06-04T14:51:59Z", "dataType": "news", "sim": 0.8941176533699036, "url": "https://www.nbcnewyork.com/news/national-international/current-and-former-openai-employees-warn-of-ais-serious-risk-and-lack-of-oversight/5475532/", "title": "Current and former OpenAI employees warn of AI's 'serious risk' and lack of oversight", "body": "OpenAI, Google, Microsoft and other companies are at the helm of a generative AI arms race -- a market that is predicted to top $1 trillion in revenue within a decade.\n\nA group of current and former OpenAI employees published an open letter Tuesday describing concerns about the artificial intelligence industry's rapid advancement despite a lack of oversight and an absence of whistleblower protections for those who wish to speak up.\n\n\"AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this,\" the employees wrote in the open letter.\n\nOpenAI, Google, Microsoft, Meta and other companies are at the helm of a generative AI arms race -- a market that is predicted to top $1 trillion in revenue within a decade -- as companies in seemingly every industry rush to add AI-powered chatbots and agents to avoid being left behind by competitors.\n\nThe current and former employees wrote AI companies have \"substantial non-public information\" about what their technology can do, the extent of the safety measures they've put in place and the risk levels that technology has for different types of harm.\n\n\"We also understand the serious risks posed by these technologies,\" they wrote. \"These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction.\"\n\n\"They currently have only weak obligations to share some of this information with governments, and none with civil society,\" the letter states. \"We do not think they can all be relied upon to share it voluntarily.\"\n\nThe letter also details the current and former employees' concerns about insufficient whistleblower protections for the AI industry, stating that without effective government oversight, employees are in a relatively unique position to hold companies accountable.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the signatories wrote. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated.\"\n\nThe letter asks AI companies to commit to not entering or enforcing non-disparagement agreements; to create anonymous processes for current and former employees to voice concerns to a company's board, regulators and others; to support a culture of open criticism; and to not retaliate against public whistleblowing if internal reporting processes fail.\n\nFour anonymous OpenAI employees and seven former ones, including Daniel Kokotajlo, Jacob Hilton, William Saunders, Carroll Wainwright and Daniel Ziegler, signed the letter. Signatories also included Ramana Kumar, who formerly worked at Google DeepMind, and Neel Nanda, who currently works at Google DeepMind and formerly worked at Anthropic. Three famed computer scientists known for advancing the artificial intelligence field also endorsed the letter: Geoffrey Hinton, Yoshua Bengio and Stuart Russell.\n\nOpenAI did not immediately respond to a request for comment. Microsoft declined to comment.\n\nLast month, OpenAI backtracked on a controversial decision to make former employees choose between signing a non-disparagement agreement that would never expire, or keeping their vested equity in the company. The internal memo, viewed by CNBC, was sent to former employees and shared with current ones.\n\nThe memo, addressed to each former employee, said that at the time of the person's departure from OpenAI, \"you may have been informed that you were required to execute a general release agreement that included a non-disparagement provision in order to retain the Vested Units [of equity].\"\n\n\"We're incredibly sorry that we're only changing this language now; it doesn't reflect our values or the company we want to be,\" an OpenAI spokesperson told CNBC at the time.\n\nTuesday's open letter also follows OpenAI's decision last month to disband its team focused on the long-term risks of AI just one year after the Microsoft-backed startup announced the group, a person familiar with the situation confirmed to CNBC at the time.\n\nThe person, who spoke on condition of anonymity, said some of the team members are being reassigned to multiple other teams within the company.\n\nThe team's disbandment followed team leaders, OpenAI co-founder Ilya Sutskever and Jan Leike, announcing their departures from the startup last month. Leike wrote in a post on X that OpenAI's \"safety culture and processes have taken a backseat to shiny products.\"\n\nCEO Sam Altman said on X he was sad to see Leike leave and that the company had more work to do. Soon after, OpenAI co-founder Greg Brockman posted a statement attributed to himself and Altman on X, asserting that the company has \"raised awareness of the risks and opportunities of AGI so that the world can better prepare for it.\"\n\n\"I joined because I thought OpenAI would be the best place in the world to do this research,\" Leike wrote on X. \"However, I have been disagreeing with OpenAI leadership about the company's core priorities for quite some time, until we finally reached a breaking point.\"\n\nLeike wrote he believes much more of the company's bandwidth should be focused on security, monitoring, preparedness, safety and societal impact.\n\n\"These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there,\" he wrote. \"Over the past few months my team has been sailing against the wind. Sometimes we were struggling for [computing resources] and it was getting harder and harder to get this crucial research done.\"\n\nLeike added that OpenAI must become a \"safety-first AGI company.\"\n\n\"Building smarter-than-human machines is an inherently dangerous endeavor,\" he wrote. \"OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.\"\n\nThe high-profile departures come months after OpenAI went through a leadership crisis involving Altman.\n\nIn November, OpenAI's board ousted Altman, saying in a statement that Altman had not been \"consistently candid in his communications with the board.\"\n\nThe issue seemed to grow more complex each day, with The Wall Street Journal and other media outlets reporting that Sutskever trained his focus on ensuring that artificial intelligence would not harm humans, while others, including Altman, were instead more eager to push ahead with delivering new technology.\n\nAltman's ouster prompted resignations or threats of resignations, including an open letter signed by virtually all of OpenAI's employees, and uproar from investors, including Microsoft. Within a week, Altman was back at the company, and board members Helen Toner, Tasha McCauley and Ilya Sutskever, who had voted to oust Altman, were out. Sutskever stayed on staff at the time but no longer in his capacity as a board member. Adam D'Angelo, who had also voted to oust Altman, remained on the board.\n\nMeanwhile, last month, OpenAI launched a new AI model and desktop version of ChatGPT, along with an updated user interface and audio capabilities, the company's latest effort to expand the use of its popular chatbot. One week after OpenAI debuted the range of audio voices, the company announced it would pull one of the viral chatbot's voices named \"Sky.\"\n\n\"Sky\" created controversy for resembling the voice of actress Scarlett Johansson in \"Her,\" a movie about artificial intelligence. The Hollywood star has alleged that OpenAI ripped off her voice even though she declined to let them use it.", "source": {"uri": "nbcnewyork.com", "dataType": "news", "title": "NBC New York"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Arms_race", "type": "wiki", "score": 5, "label": {"eng": "Arms race"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 5, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 5, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 5, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Meta_Platforms", "type": "org", "score": 3, "label": {"eng": "Meta Platforms"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 3, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/CNBC", "type": "org", "score": 3, "label": {"eng": "CNBC"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 2, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 2, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 2, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Startup_company", "type": "wiki", "score": 2, "label": {"eng": "Startup company"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 1, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Adam_D'Angelo", "type": "person", "score": 1, "label": {"eng": "Adam D'Angelo"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Bandwidth_(computing)", "type": "wiki", "score": 1, "label": {"eng": "Bandwidth (computing)"}}, {"uri": "http://en.wikipedia.org/wiki/Desktop_computer", "type": "wiki", "score": 1, "label": {"eng": "Desktop computer"}}, {"uri": "http://en.wikipedia.org/wiki/Computing", "type": "wiki", "score": 1, "label": {"eng": "Computing"}}, {"uri": "http://en.wikipedia.org/wiki/Viral_phenomenon", "type": "wiki", "score": 1, "label": {"eng": "Viral phenomenon"}}, {"uri": "http://en.wikipedia.org/wiki/User_interface", "type": "wiki", "score": 1, "label": {"eng": "User interface"}}, {"uri": "http://en.wikipedia.org/wiki/The_Wall_Street_Journal", "type": "wiki", "score": 1, "label": {"eng": "The Wall Street Journal"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 18}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 19}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 17}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 17}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 72}], "image": "https://media.nbcnewyork.com/2019/09/NBC@3x-7-1.png?fit=5761%2C3240&quality=85&strip=all", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.03529411764705892, "wgt": 228, "relevance": 1}
{"uri": "8161312505", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "14:43:21", "dateTime": "2024-06-04T14:43:21Z", "dateTimePub": "2024-06-04T14:42:18Z", "dataType": "news", "sim": 0.8823529481887817, "url": "https://www.cnbc.com/2024/06/04/openai-open-ai-risks-lack-of-oversight.html", "title": "Current and former OpenAI employees warn of AI's 'serious risk' and lack of oversight", "body": "OpenAI CEO Sam Altman speaks during the Microsoft Build conference at Microsoft headquarters in Redmond, Washington, on May 21, 2024.\n\nA group of current and former OpenAI employees published an open letter Tuesday describing concerns about the artificial intelligence industry's rapid advancement despite a lack of oversight and an absence of whistleblower protections for those who wish to speak up.\n\n\"AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this,\" the employees wrote in the open letter.\n\nOpenAI, Google, Microsoft, Meta and other companies are at the helm of a generative AI arms race -- a market that is predicted to top $1 trillion in revenue within a decade -- as companies in seemingly every industry rush to add AI-powered chatbots and agents to avoid being left behind by competitors.\n\nThe current and former employees wrote AI companies have \"substantial non-public information\" about what their technology can do, the extent of the safety measures they've put in place and the risk levels that technology has for different types of harm.\n\n\"We also understand the serious risks posed by these technologies,\" they wrote, adding that the companies \"currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThe letter also details the current and former employees' concerns about insufficient whistleblower protections for the AI industry, stating that without effective government oversight, employees are in a relatively unique position to hold companies accountable.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the signatories wrote. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated.\"\n\nThe letter asks AI companies to commit to not entering or enforcing non-disparagement agreements; to create anonymous processes for current and former employees to voice concerns to a company's board, regulators and others; to support a culture of open criticism; and to not retaliate against public whistleblowing if internal reporting processes fail.\n\nFour anonymous OpenAI employees and seven former ones, including Daniel Kokotajlo, Jacob Hilton, William Saunders, Carroll Wainwright and Daniel Ziegler, signed the letter. Signatories also included Ramana Kumar, who formerly worked at Google DeepMind, and Neel Nanda, who currently works at Google DeepMind and formerly worked at Anthropic. Three famed computer scientists known for advancing the artificial intelligence field also endorsed the letter: Geoffrey Hinton, Yoshua Bengio and Stuart Russell.\n\n\"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world,\" an OpenAI spokesperson told CNBC, adding that the company has an anonymous integrity hotline, as well as a Safety and Security Committee led by members of the board and OpenAI leaders.", "source": {"uri": "cnbc.com", "dataType": "news", "title": "CNBC"}, "authors": [{"uri": "hayden_field@cnbc.com", "name": "Hayden Field", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 5, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 5, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 4, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 3, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Meta_Platforms", "type": "org", "score": 3, "label": {"eng": "Meta Platforms"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 3, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Arms_race", "type": "wiki", "score": 3, "label": {"eng": "Arms race"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 3, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 3, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Redmond,_Washington", "type": "loc", "score": 3, "label": {"eng": "Redmond, Washington"}, "location": {"type": "place", "label": {"eng": "Redmond, Washington"}, "country": {"type": "country", "label": {"eng": "United States"}}}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 2, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 1, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 1, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/CNBC", "type": "org", "score": 1, "label": {"eng": "CNBC"}}], "categories": [{"uri": "dmoz/Society/Work", "label": "dmoz/Society/Work", "wgt": 17}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 30}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 24}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 17}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 18}, {"uri": "news/Business", "label": "news/Business", "wgt": 56}], "image": "https://image.cnbcfm.com/api/v1/image/107418094-1716321352537-gettyimages-2153474140-AFP_34TH9TC.jpeg?v=1717509593&w=1920&h=1080", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.1215686274509804, "wgt": 225, "relevance": 1}
{"uri": "2024-06-378680254", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:49:05", "dateTime": "2024-06-04T16:49:05Z", "dateTimePub": "2024-06-04T16:36:25Z", "dataType": "news", "sim": 0.8235294222831726, "url": "https://www.businessinsider.in/tech/news/openai-employees-are-demanding-change-here-are-the-4-things-they-want-/articleshow/110711256.cms", "title": "OpenAI employees are demanding change. Here are the 4 things they want. | Business Insider India", "body": "Sam Altman and other leaders of AI are under fire by former and current employees. Jack Guez/Getty Images; Jenny Chang-Rodriguez/BI\n\nA group of nine current and former OpenAI employees signed a letter calling out tech firms over major concerns about the risks of artificial intelligence.\n\nIn their letter, the tech workers called for more transparency in AI companies and better protections for whistleblowers who wish to raise concerns about the power of AI.\n\n\"We are current and former employees at frontier AI companies, and we believe in the potential of AI technology to deliver unprecedented benefits to humanity,\" the letter said.\n\n\"We also understand the serious risks posed by these technologies. These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction,\" it continued. \"AI companies themselves have acknowledged these risks, as have governments across the world, and other AI experts.\"\n\nA total of 13 people signed the letter, and they all come from some of the top players in AI -- including OpenAI, Anthropic, and DeepMind. It was also endorsed by two men known as the \"Godfathers of AI,\" Yoshua Bengio and Geoffrey Hinton.\n\n\"I decided to leave OpenAI because I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence,\" former OpenAI employee Daniel Kokotajlo said in a statement.\n\n\"They and others have bought into the 'move fast and break things' approach and that is the opposite of what is needed for technology this powerful and this poorly understood,\" he added.\n\nThe AI employees outlined a list of four demands that they said would help mitigate the existing issues of inequality and misinformation in the AI space.\n\nHere's a look at the four principles the 13 employees said they want OpenAI and other AI companies to adopt, according to the letter.\n\nBusiness Insider has reached out to OpenAI, Antrhopic, and Google Deepmind for comment on the letter.\n\nOpenAI spokesperson Lindsey Held told The New York Times that the company is \"proud of our track record providing the most capable and safest A.I. systems and believe in our scientific approach to addressing risk.\"\n\n\"We agree that rigorous debate is crucial given the significance of this technology, and we'll continue to engage with governments, civil society and other communities around the world,\" the statement continued.", "source": {"uri": "businessinsider.in", "dataType": "news", "title": "Business Insider India"}, "authors": [{"uri": "jordan_hart@businessinsider.in", "name": "Jordan Hart", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 3, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Business_Insider", "type": "wiki", "score": 3, "label": {"eng": "Business Insider"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 3, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 2, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 2, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 2, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 1, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 1, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Economic_inequality", "type": "wiki", "score": 1, "label": {"eng": "Economic inequality"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 1, "label": {"eng": "The New York Times"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 16}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 28}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 22}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 47}], "image": "https://staticbiassets.in/thumb/msid-110710612,width-700,resizemode-4,imgsize-253456/img664f6822a961b37edf394160.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.07450980392156858, "wgt": 210, "relevance": 1}
{"uri": "8161369112", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:20:22", "dateTime": "2024-06-04T15:20:22Z", "dateTimePub": "2024-06-04T15:19:26Z", "dataType": "news", "sim": 0.8196078538894653, "url": "https://www.wired.com/story/openai-right-to-warn-open-letter-ai-risk/", "title": "OpenAI Employees Warn of a Culture of Risk and Retaliation", "body": "An open letter signed by former and current employees at OpenAI and other AI giants calls for whistleblower protections as the artificial intelligence rapidly evolves.\n\nA group of current and former OpenAI employees have issued a public letter warning that the company and its rivals are building artificial intelligence with undue risk, without sufficient oversight, and while muzzling employees who might witness irresponsible activities.\n\n\"These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction,\" reads the letter published at righttowarn.ai. \"So long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable.\"\n\nThe letter calls for not just OpenAI but all AI companies to commit to not punishing employees who speak out about their activities. It also calls for companies to establish \"verifiable\" ways for workers to provide anonymous feedback on their activities. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated,\" the letter reads. \"Some of us reasonably fear various forms of retaliation, given the history of such cases across the industry.\"\n\nOpenAI came under criticism last month after a Vox article revealed that the company has threatened to claw back employees' equity if they do not sign non-disparagement agreements that forbid them from criticizing the company or even mentioning the existence of such an agreement. OpenAI's CEO, Sam Altman, said on X recently that he was unaware of such arrangements and the company had never clawed back anyone's equity. Altman also said the clause would be removed, freeing employees to speak out. OpenAI did not respond to a request for comment by time of posting.\n\nOpenAI has also recently changed its approach to managing safety. Last month an OpenAI research group responsible for assessing and countering the long-term risks posed by the company's more powerful AI models was effectively dissolved after several prominent figures left and the remaining members of the team were absorbed into other groups. A few weeks later, the company announced that it had created a Safety and Security Committee, led by Altman and other board members.\n\nLast November, Altman was fired by OpenAI's board for allegedly failing to disclose information and deliberately misleading them. After a very public tussle, Altman returned to the company and most of the board was ousted.\n\nThe letters' signatories include people who worked on safety and governance at OpenAI, current employees who signed anonymously, and researchers who currently work at rival AI companies. It was also endorsed by several big name AI researchers including Geoffrey Hinton and Yoshua Bengio, who both won the Turing Award for pioneering AI research, and Stuart Russell a leading expert on AI safety.\n\nFormer employees to have signed the letter include William Saunders, Carroll Wainwright, Daniel Ziegler, all of whom worked on AI safety at OpenAI.\n\n\"The public at large is currently underestimating the pace at which this technology is developing,\" says Jacob Hilton, a researcher who previously worked on reinforcement learning at OpenAI and who left the company more than a year ago to pursue a new research opportunity. Hilton says although companies like OpenAI commit to building AI safety there is little oversight to ensure that is the case. \"The protections that we're asking for, they're intended to apply to all frontier AI companies, not just OpenAI,\" he says.\n\n\"I left because I lost confidence that OpenAI would behave responsibly,\" says Daniel Kokotajlo, a researcher who previously worked on AI governance at OpenAI. \"There are things that happened that I think should have been disclosed to the public,\" he adds, declining to provide specifics.\n\nKokotajlo says the letter's proposal would provide greater transparency and he believes there's a good chance that OpenAI and others will reform their policies given the negative reaction to news of non-disparagement agreements. He also says that AI is advancing with worrying speed. \"The stakes are going to get much, much, much higher in the next few years, he says, \"at least so I believe.\"", "source": {"uri": "wired.com", "dataType": "news", "title": "Wired"}, "authors": [{"uri": "conde_nast@wired.com", "name": "Cond\u00e9 Nast", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 3, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 3, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 2, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Vox_(website)", "type": "wiki", "score": 2, "label": {"eng": "Vox (website)"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 2, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 2, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Reinforcement_learning", "type": "wiki", "score": 1, "label": {"eng": "Reinforcement learning"}}, {"uri": "http://en.wikipedia.org/wiki/Turing_Award", "type": "wiki", "score": 1, "label": {"eng": "Turing Award"}}], "categories": [{"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 24}, {"uri": "dmoz/Business/Human_Resources/Training_and_Safety", "label": "dmoz/Business/Human Resources/Training and Safety", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 22}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 24}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 20}, {"uri": "news/Business", "label": "news/Business", "wgt": 46}], "image": "https://media.wired.com/photos/665f2c61d5fa7848dbc4d268/191:100/w_1280,c_limit/OpenAI-Employees-Warn-of-a-Culture-of-Risk-and-Retaliation-GettyImages-1913209938.jpg?mbid=social_retweet", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.1607843137254902, "wgt": 209, "relevance": 1}
{"uri": "8161468199", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:28:33", "dateTime": "2024-06-04T16:28:33Z", "dateTimePub": "2024-06-04T16:26:56Z", "dataType": "news", "sim": 0.8078431487083435, "url": "https://www.startribune.com/former-openai-employees-lead-push-to-protect-whistleblowers-flagging-artificial-intelligence-risks/600370934/", "title": "Former OpenAI employees lead push to protect whistleblowers flagging artificial intelligence risks", "body": "A group of OpenAI's current and former workers is calling on the ChatGPT-maker and other artificial intelligence companies to protect whistleblowing employees who flag safety risks about AI technology.\n\nA group of OpenAI's current and former workers is calling on the ChatGPT-maker and other artificial intelligence companies to protect whistleblowing employees who flag safety risks about AI technology.\n\nAn open letter published Tuesday asks tech companies to establish stronger whistleblower protections so researchers can raise concerns about the development of high-performing AI systems internally and with the public without fear of retaliation.\n\nFormer OpenAI employee Daniel Kokotajlo, who left the company earlier this year, said in a written statement that tech companies are ''disregarding the risks and impact of AI'' as they race to develop better-than-human AI systems known as artificial general intelligence.\n\n''I decided to leave OpenAI because I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence,\" he wrote. \"They and others have bought into the 'move fast and break things' approach and that is the opposite of what is needed for technology this powerful and this poorly understood.''\n\nOpenAI said in a statement responding to the letter that it already has measures for employees to express concerns, including an anonymous integrity hotline.\n\n''We're proud of our track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk,\" said the company's statement. \"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.''\n\nThe letter has 13 signatories, most of whom are former employees of OpenAI and two who work or worked for Google's DeepMind. Four are listed as anonymous current employees of OpenAI. The letter asks that companies stop making workers enter into ''non-disparagement'' agreements that can punish them if they criticize the company.\n\nSocial media outrage over language in OpenAI's paperwork for departing workers recently led the company to release all its former employees from their non-disparagement agreements.\n\nThe open letter has the support of pioneering AI scientists Yoshua Bengio and Geoffrey Hinton, who together won computer science's highest award, and Stuart Russell. All three have warned about the risks that future AI systems could pose to humanity's existence.\n\nThe letter comes as OpenAI has said it is beginning to develop the next generation of the AI technology behind ChatGPT and forming a new safety committee just after losing a set of leaders, including co-founder Ilya Sutskever, who were part of a team focused on safely developing the most powerful AI systems. The broader AI research community has long battled over the gravity of AI's short-term and long-term risks and how to square them with the technology's commercialization.\n\nThose conflicts contributed to the ouster, and swift return, of OpenAI CEO Sam Altman last year, and continue to fuel distrust in his leadership.\n\nMore recently, a new product showcase drew the ire of Hollywood star Scarlett Johansson, who said she was shocked to hear ChatGPT's voice sounding ''eerily similar'' to her own despite having previously rejected Altman's request that she lend her voice to the system.\n\n-- -- -\n\nThe Associated Press and OpenAI have a licensing and technology agreement that allows OpenAI access to part of AP's text archives.", "source": {"uri": "startribune.com", "dataType": "news", "title": "Star Tribune"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 4, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 2, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 1, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 1, "label": {"eng": "Scarlett Johansson"}}, {"uri": "http://en.wikipedia.org/wiki/Associated_Press", "type": "org", "score": 1, "label": {"eng": "Associated Press"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Social_media", "type": "wiki", "score": 1, "label": {"eng": "Social media"}}], "categories": [{"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 34}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 29}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 28}], "image": "https://www.startribune.com/static/img/branding/logos/strib-social-card.png?d=1717510706", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.5294117647058822, "wgt": 206, "relevance": 1}
{"uri": "2024-06-378571062", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:05:17", "dateTime": "2024-06-04T15:05:17Z", "dateTimePub": "2024-06-04T15:05:13Z", "dataType": "news", "sim": 0.8078431487083435, "url": "https://www.nbcdfw.com/news/business/money-report/current-and-former-openai-employees-warn-of-ais-serious-risk-and-lack-of-oversight/3558720/", "title": "Current and former OpenAI employees warn of AI's 'serious risk' and lack of oversight", "body": "A group of current and former OpenAI employees published an open letter Tuesday describing concerns about the artificial intelligence industry's rapid advancement despite a lack of oversight and an absence of whistleblower protections for those who wish to speak up.\n\n\"AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this,\" the employees wrote in the open letter.\n\nOpenAI, Google, Microsoft, Meta and other companies are at the helm of a generative AI arms race -- a market that is predicted to top $1 trillion in revenue within a decade -- as companies in seemingly every industry rush to add AI-powered chatbots and agents to avoid being left behind by competitors.\n\nThe current and former employees wrote AI companies have \"substantial non-public information\" about what their technology can do, the extent of the safety measures they've put in place and the risk levels that technology has for different types of harm.\n\n\"We also understand the serious risks posed by these technologies,\" they wrote, adding that the companies \"currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThe letter also details the current and former employees' concerns about insufficient whistleblower protections for the AI industry, stating that without effective government oversight, employees are in a relatively unique position to hold companies accountable.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the signatories wrote. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated.\"\n\nThe letter asks AI companies to commit to not entering or enforcing non-disparagement agreements; to create anonymous processes for current and former employees to voice concerns to a company's board, regulators and others; to support a culture of open criticism; and to not retaliate against public whistleblowing if internal reporting processes fail.\n\nFour anonymous OpenAI employees and seven former ones, including Daniel Kokotajlo, Jacob Hilton, William Saunders, Carroll Wainwright and Daniel Ziegler, signed the letter. Signatories also included Ramana Kumar, who formerly worked at Google DeepMind, and Neel Nanda, who currently works at Google DeepMind and formerly worked at Anthropic. Three famed computer scientists known for advancing the artificial intelligence field also endorsed the letter: Geoffrey Hinton, Yoshua Bengio and Stuart Russell.\n\n\"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world,\" an OpenAI spokesperson told CNBC, adding that the company has an anonymous integrity hotline, as well as a Safety and Security Committee led by members of the board and OpenAI leaders.\n\nMicrosoft declined to comment.\n\nLast month, OpenAI backtracked on a controversial decision to make former employees choose between signing a non-disparagement agreement that would never expire, or keeping their vested equity in the company. The internal memo, viewed by CNBC, was sent to former employees and shared with current ones.\n\nThe memo, addressed to each former employee, said that at the time of the person's departure from OpenAI, \"you may have been informed that you were required to execute a general release agreement that included a non-disparagement provision in order to retain the Vested Units [of equity].\"\n\n\"We're incredibly sorry that we're only changing this language now; it doesn't reflect our values or the company we want to be,\" an OpenAI spokesperson told CNBC at the time.\n\nTuesday's open letter also follows OpenAI's decision last month to disband its team focused on the long-term risks of AI just one year after the Microsoft-backed startup announced the group, a person familiar with the situation confirmed to CNBC at the time.\n\nThe person, who spoke on condition of anonymity, said some of the team members are being reassigned to multiple other teams within the company.\n\nThe team's disbandment followed team leaders, OpenAI co-founder Ilya Sutskever and Jan Leike, announcing their departures from the startup last month. Leike wrote in a post on X that OpenAI's \"safety culture and processes have taken a backseat to shiny products.\"\n\nCEO Sam Altman said on X he was sad to see Leike leave and that the company had more work to do. Soon after, OpenAI co-founder Greg Brockman posted a statement attributed to himself and Altman on X, asserting that the company has \"raised awareness of the risks and opportunities of AGI so that the world can better prepare for it.\"\n\n\"I joined because I thought OpenAI would be the best place in the world to do this research,\" Leike wrote on X. \"However, I have been disagreeing with OpenAI leadership about the company's core priorities for quite some time, until we finally reached a breaking point.\"\n\nLeike wrote he believes much more of the company's bandwidth should be focused on security, monitoring, preparedness, safety and societal impact.\n\n\"These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there,\" he wrote. \"Over the past few months my team has been sailing against the wind. Sometimes we were struggling for [computing resources] and it was getting harder and harder to get this crucial research done.\"\n\nLeike added that OpenAI must become a \"safety-first AGI company.\"\n\n\"Building smarter-than-human machines is an inherently dangerous endeavor,\" he wrote. \"OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.\"\n\nThe high-profile departures come months after OpenAI went through a leadership crisis involving Altman.\n\nIn November, OpenAI's board ousted Altman, saying in a statement that Altman had not been \"consistently candid in his communications with the board.\"\n\nThe issue seemed to grow more complex each day, with The Wall Street Journal and other media outlets reporting that Sutskever trained his focus on ensuring that artificial intelligence would not harm humans, while others, including Altman, were instead more eager to push ahead with delivering new technology.\n\nAltman's ouster prompted resignations or threats of resignations, including an open letter signed by virtually all of OpenAI's employees, and uproar from investors, including Microsoft. Within a week, Altman was back at the company, and board members Helen Toner, Tasha McCauley and Ilya Sutskever, who had voted to oust Altman, were out. Sutskever stayed on staff at the time but no longer in his capacity as a board member. Adam D'Angelo, who had also voted to oust Altman, remained on the board.\n\nMeanwhile, last month, OpenAI launched a new AI model and desktop version of ChatGPT, along with an updated user interface and audio capabilities, the company's latest effort to expand the use of its popular chatbot. One week after OpenAI debuted the range of audio voices, the company announced it would pull one of the viral chatbot's voices named \"Sky.\"\n\n\"Sky\" created controversy for resembling the voice of actress Scarlett Johansson in \"Her,\" a movie about artificial intelligence. The Hollywood star has alleged that OpenAI ripped off her voice even though she declined to let them use it.", "source": {"uri": "nbcdfw.com", "dataType": "news", "title": "NBC 5 Dallas-Fort Worth"}, "authors": [{"uri": "hayden_field@nbcdfw.com", "name": "Hayden Field", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 5, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/CNBC", "type": "org", "score": 4, "label": {"eng": "CNBC"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 4, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 3, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Meta_Platforms", "type": "org", "score": 3, "label": {"eng": "Meta Platforms"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Arms_race", "type": "wiki", "score": 3, "label": {"eng": "Arms race"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 3, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 3, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 2, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 2, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Startup_company", "type": "wiki", "score": 2, "label": {"eng": "Startup company"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 1, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Adam_D'Angelo", "type": "person", "score": 1, "label": {"eng": "Adam D'Angelo"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Bandwidth_(computing)", "type": "wiki", "score": 1, "label": {"eng": "Bandwidth (computing)"}}, {"uri": "http://en.wikipedia.org/wiki/Desktop_computer", "type": "wiki", "score": 1, "label": {"eng": "Desktop computer"}}, {"uri": "http://en.wikipedia.org/wiki/Computing", "type": "wiki", "score": 1, "label": {"eng": "Computing"}}, {"uri": "http://en.wikipedia.org/wiki/Viral_phenomenon", "type": "wiki", "score": 1, "label": {"eng": "Viral phenomenon"}}, {"uri": "http://en.wikipedia.org/wiki/User_interface", "type": "wiki", "score": 1, "label": {"eng": "User interface"}}, {"uri": "http://en.wikipedia.org/wiki/The_Wall_Street_Journal", "type": "wiki", "score": 1, "label": {"eng": "The Wall Street Journal"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Society/Work", "label": "dmoz/Society/Work", "wgt": 16}, {"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 18}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 20}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 21}, {"uri": "dmoz/Business/Human_Resources/Training_and_Safety", "label": "dmoz/Business/Human Resources/Training and Safety", "wgt": 16}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 64}], "image": "https://media.nbcdfw.com/2024/06/107418094-1716321352537-gettyimages-2153474140-AFP_34TH9TC.jpeg?quality=85&strip=all&resize=1200%2C675", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.2313725490196079, "wgt": 206, "relevance": 1}
{"uri": "8161515015", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:04:03", "dateTime": "2024-06-04T17:04:03Z", "dateTimePub": "2024-06-04T17:01:59Z", "dataType": "news", "sim": 0.8039215803146362, "url": "https://www.nbcboston.com/news/business/money-report/current-and-former-openai-employees-warn-of-ais-serious-risk-and-lack-of-oversight/3388761/", "title": "Current and former OpenAI employees warn of AI's 'serious risks' and lack of oversight", "body": "OpenAI, Google, Microsoft and other companies are at the helm of a generative AI arms race -- a market that is predicted to top $1 trillion in revenue within a decade.\n\nA group of current and former OpenAI employees published an open letter Tuesday describing concerns about the artificial intelligence industry's rapid advancement despite a lack of oversight and an absence of whistleblower protections for those who wish to speak up.\n\n\"AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this,\" the employees wrote.\n\nOpenAI, Google, Microsoft, Meta and other companies are at the helm of a generative AI arms race -- a market that is predicted to top $1 trillion in revenue within a decade -- as companies in seemingly every industry rush to add AI-powered chatbots and agents to avoid being left behind by competitors.\n\nThe current and former employees wrote that AI companies have \"substantial non-public information\" about what their technology can do, the extent of the safety measures they've put in place and the risk levels that technology has for different types of harm.\n\n\"We also understand the serious risks posed by these technologies,\" they wrote, adding that the companies \"currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThe letter also details the current and former employees' concerns about insufficient whistleblower protections for the AI industry, saying that without effective government oversight, employees are in a relatively unique position to hold companies accountable.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the signatories wrote. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated.\"\n\nThe letter asks AI companies to commit to not entering or enforcing non-disparagement agreements; to create anonymous processes for current and former employees to voice concerns to a company's board, regulators and others; to support a culture of open criticism; and to not retaliate against public whistleblowing if internal reporting processes fail.\n\nFour anonymous OpenAI employees and seven former ones, including Daniel Kokotajlo, Jacob Hilton, William Saunders, Carroll Wainwright and Daniel Ziegler, signed the letter. Signatories also included Ramana Kumar, who formerly worked at Google DeepMind, and Neel Nanda, who currently works at Google DeepMind and formerly worked at Anthropic. Three famed computer scientists known for advancing the artificial intelligence field also endorsed the letter: Geoffrey Hinton, Yoshua Bengio and Stuart Russell.\n\nAn OpenAI spokesperson told CNBC: \"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\" The company, which is backed by Microsoft, has an anonymous integrity hotline, as well as a Safety and Security Committee led by members of the board and OpenAI leaders, the spokesperson said.\n\nMicrosoft declined to comment.\n\nIn May, OpenAI backtracked on a controversial decision to make former employees choose between signing a non-disparagement agreement that would never expire and keeping their vested equity in the company. An internal memo, viewed by CNBC, was sent to former employees and shared with current ones.\n\nThe memo, addressed to each former employee, said that at the time of the person's departure from OpenAI, \"you may have been informed that you were required to execute a general release agreement that included a non-disparagement provision in order to retain the Vested Units [of equity].\"\n\n\"We're incredibly sorry that we're only changing this language now; it doesn't reflect our values or the company we want to be,\" an OpenAI spokesperson told CNBC at the time.\n\nTuesday's open letter also follows OpenAI's decision in May to disband its team focused on the long-term risks of AI just one year after it announced the group, a person familiar with the situation confirmed to CNBC at the time.\n\nThe person, who spoke on condition of anonymity, said some of the team members are being reassigned to other teams within the company.\n\nThe team was disbanded after its leaders, OpenAI co-founder Ilya Sutskever and Jan Leike, announced their departures from the startup in May. Leike wrote in a post on X that OpenAI's \"safety culture and processes have taken a backseat to shiny products.\"\n\nCEO Sam Altman said on X he was sad to see Leike leave and that the company had more work to do. Soon after, OpenAI co-founder Greg Brockman posted a statement attributed to Brockman and Altman on X, asserting that the company has \"raised awareness of the risks and opportunities of AGI so that the world can better prepare for it.\"\n\n\"I joined because I thought OpenAI would be the best place in the world to do this research,\" Leike wrote on X. \"However, I have been disagreeing with OpenAI leadership about the company's core priorities for quite some time, until we finally reached a breaking point.\"\n\nLeike wrote that he believes much more of the company's bandwidth should be focused on security, monitoring, preparedness, safety and societal impact.\n\n\"These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there,\" he wrote. \"Over the past few months my team has been sailing against the wind. Sometimes we were struggling for [computing resources] and it was getting harder and harder to get this crucial research done.\"\n\nLeike added that OpenAI must become a \"safety-first AGI company.\"\n\n\"Building smarter-than-human machines is an inherently dangerous endeavor,\" he wrote. \"OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.\"\n\nThe high-profile departures come months after OpenAI went through a leadership crisis involving Altman.\n\nIn November, OpenAI's board ousted Altman, saying in a statement that Altman had not been \"consistently candid in his communications with the board.\"\n\nThe issue seemed to grow more complex each day, with The Wall Street Journal and other media outlets reporting that Sutskever trained his focus on ensuring that artificial intelligence would not harm humans, while others, including Altman, were instead more eager to push ahead with delivering new technology.\n\nAltman's ouster prompted resignations or threats of resignations, including an open letter signed by virtually all of OpenAI's employees, and uproar from investors, including Microsoft. Within a week, Altman was back at the company, and board members Helen Toner, Tasha McCauley and Sutskever, who had voted to oust Altman, were out. Sutskever stayed on staff at the time but no longer in his capacity as a board member. Adam D'Angelo, who had also voted to oust Altman, remained on the board.\n\nIn May, OpenAI launched a new AI model and desktop version of ChatGPT, along with an updated user interface and audio capabilities, the company's latest effort to expand the use of its popular chatbot. One week after OpenAI debuted the range of audio voices, the company announced it would pull one of the viral chatbot's voices, named \"Sky.\"\n\n\"Sky\" created controversy because it resembled the voice of actress Scarlett Johansson in \"Her,\" a movie about artificial intelligence. The Hollywood star has alleged that OpenAI ripped off her voice even though she declined to let the company use it.", "source": {"uri": "nbcboston.com", "dataType": "news", "title": "NBC Boston"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Arms_race", "type": "wiki", "score": 5, "label": {"eng": "Arms race"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 5, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 5, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 5, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/CNBC", "type": "org", "score": 4, "label": {"eng": "CNBC"}}, {"uri": "http://en.wikipedia.org/wiki/Meta_Platforms", "type": "org", "score": 3, "label": {"eng": "Meta Platforms"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 3, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 2, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 2, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 1, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Adam_D'Angelo", "type": "person", "score": 1, "label": {"eng": "Adam D'Angelo"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Bandwidth_(computing)", "type": "wiki", "score": 1, "label": {"eng": "Bandwidth (computing)"}}, {"uri": "http://en.wikipedia.org/wiki/Desktop_computer", "type": "wiki", "score": 1, "label": {"eng": "Desktop computer"}}, {"uri": "http://en.wikipedia.org/wiki/Computing", "type": "wiki", "score": 1, "label": {"eng": "Computing"}}, {"uri": "http://en.wikipedia.org/wiki/Viral_phenomenon", "type": "wiki", "score": 1, "label": {"eng": "Viral phenomenon"}}, {"uri": "http://en.wikipedia.org/wiki/User_interface", "type": "wiki", "score": 1, "label": {"eng": "User interface"}}, {"uri": "http://en.wikipedia.org/wiki/Startup_company", "type": "wiki", "score": 1, "label": {"eng": "Startup company"}}, {"uri": "http://en.wikipedia.org/wiki/The_Wall_Street_Journal", "type": "wiki", "score": 1, "label": {"eng": "The Wall Street Journal"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 18}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 20}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 21}, {"uri": "dmoz/Business/Human_Resources/Training_and_Safety", "label": "dmoz/Business/Human Resources/Training and Safety", "wgt": 17}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 16}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 64}], "image": "https://media.nbcboston.com/2024/06/107418094-1716321352537-gettyimages-2153474140-AFP_34TH9TC.jpeg?quality=85&strip=all&resize=1200%2C675", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.1529411764705881, "wgt": 205, "relevance": 1}
{"uri": "2024-06-378663741", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:32:11", "dateTime": "2024-06-04T16:32:11Z", "dateTimePub": "2024-06-04T16:27:09Z", "dataType": "news", "sim": 0.7921568751335144, "url": "https://www.france24.com/en/live-news/20240604-openai-insiders-blast-lack-of-ai-transparency", "title": "OpenAI insiders blast lack of AI transparency", "body": "San Francisco (AFP) - A group of current and former employees from OpenAI on Tuesday issued an open letter warning that the world's leading artificial intelligence companies were falling short of necessary transparency and accountability to meet the potential risks posed by the technology.\n\nThe letter raised serious concerns about AI safety risks \"ranging from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction.\"\n\nThe 16 signatories, which also included a staff member from Google DeepMind, warned that AI companies \"have strong financial incentives to avoid effective oversight\" and that self-regulation by the companies would not effectively change this.\n\n\"AI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm,\" the letter said.\n\n\"However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThat reality, the letter added, meant that employees inside the companies were the only ones who could notify the public, and the signatories called for broader whistleblower laws to protect them.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the letter said.\n\nThe four current employees of OpenAI signed the letter anonymously because they feared retaliation from the company, The New York Times reported.\n\nIt was also signed by Yoshua Bengio, Geoffrey Hinton and Stuart Russell, who are often described as AI \"godfathers\" and have criticized the lack of preparation for AI's dangers.\n\nOpenAI in a statement pushed back at the criticism.\n\n\"We're proud of our track record of providing the most capable and safest AI systems and believe in our scientific approach to addressing risk,\" a statement said.\n\n\"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"\n\nOpenAI also said it had \"avenues for employees to express their concerns including an anonymous integrity hotline\" and a newly formed Safety and Security Committee led by members of the board and executives, including CEO Sam Altman.\n\nThe criticism of OpenAI, which was first released to the Times, comes as questions are growing around Altman's leadership of the company.\n\nOpenAI has unveiled a wave of new products, though the company insists they will only get released to the public after thorough testing.\n\nAn unveiling of a human-like chatbot caused a controversy when Hollywood star Scarlett Johansson complained that it closely resembled her voice.\n\nShe had previously turned down an offer from Altman to work with the company.", "source": {"uri": "france24.com", "dataType": "news", "title": "France 24"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 3, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Agence_France-Presse", "type": "wiki", "score": 3, "label": {"eng": "Agence France-Presse"}}, {"uri": "http://en.wikipedia.org/wiki/San_Francisco", "type": "loc", "score": 3, "label": {"eng": "San Francisco"}, "location": {"type": "place", "label": {"eng": "San Francisco"}, "country": {"type": "country", "label": {"eng": "United States"}}}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 2, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 2, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Hollywood,_Los_Angeles", "type": "wiki", "score": 1, "label": {"eng": "Hollywood, Los Angeles"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 1, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 1, "label": {"eng": "Scarlett Johansson"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 1, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 16}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 17}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 19}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 67}], "image": "https://s.france24.com/media/display/499481e4-228f-11ef-bcb5-005056a97e36/w:1280/p:16x9/0ef1c79425d19f454c3e1dc35623dc24040e4092.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.2705882352941177, "wgt": 202, "relevance": 1}
{"uri": "8161476693", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:34:03", "dateTime": "2024-06-04T16:34:03Z", "dateTimePub": "2024-06-04T16:33:20Z", "dataType": "news", "sim": 0.7921568751335144, "url": "https://www.digitaljournal.com/tech-science/openai-insiders-blast-lack-of-ai-transparency/article", "title": "OpenAI insiders blast lack of AI transparency", "body": "The open letter criticizing AI transparency comes amid questions about OpenAI CEO Sam Altman's corporate leadership - Copyright AFP/File Jason Redmond\n\nA group of current and former employees from OpenAI on Tuesday issued an open letter warning that the world's leading artificial intelligence companies were falling short of necessary transparency and accountability to meet the potential risks posed by the technology.\n\nThe letter raised serious concerns about AI safety risks \"ranging from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction.\"\n\nThe 16 signatories, which also included a staff member from Google DeepMind, warned that AI companies \"have strong financial incentives to avoid effective oversight\" and that self-regulation by the companies would not effectively change this.\n\n\"AI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm,\" the letter said.\n\n\"However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThat reality, the letter added, meant that employees inside the companies were the only ones who could notify the public, and the signatories called for broader whistleblower laws to protect them.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the letter said.\n\nThe four current employees of OpenAI signed the letter anonymously because they feared retaliation from the company, The New York Times reported.\n\nIt was also signed by Yoshua Bengio, Geoffrey Hinton and Stuart Russell, who are often described as AI \"godfathers\" and have criticized the lack of preparation for AI's dangers.\n\nOpenAI in a statement pushed back at the criticism.\n\n\"We're proud of our track record of providing the most capable and safest AI systems and believe in our scientific approach to addressing risk,\" a statement said.\n\n\"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"\n\nOpenAI also said it had \"avenues for employees to express their concerns including an anonymous integrity hotline\" and a newly formed Safety and Security Committee led by members of the board and executives, including CEO Sam Altman.\n\nThe criticism of OpenAI, which was first released to the Times, comes as questions are growing around Altman's leadership of the company.\n\nOpenAI has unveiled a wave of new products, though the company insists they will only get released to the public after thorough testing.\n\nAn unveiling of a human-like chatbot caused a controversy when Hollywood star Scarlett Johansson complained that it closely resembled her voice.\n\nShe had previously turned down an offer from Altman to work with the company.", "source": {"uri": "digitaljournal.com", "dataType": "news", "title": "Digital Journal"}, "authors": [{"uri": "agence_france_presse@digitaljournal.com", "name": "Agence France-Presse", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 5, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 3, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 3, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 3, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 2, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 2, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Hollywood,_Los_Angeles", "type": "wiki", "score": 1, "label": {"eng": "Hollywood, Los Angeles"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 1, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 1, "label": {"eng": "Scarlett Johansson"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 1, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/Agence_France-Presse", "type": "wiki", "score": 1, "label": {"eng": "Agence France-Presse"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 15}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 15}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 17}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 20}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 62}], "image": "https://www.digitaljournal.com/wp-content/uploads/2024/06/08c896ae18c48dad4aaee47bbaf863548f50d895.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.0980392156862745, "wgt": 202, "relevance": 1}
{"uri": "8161463535", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:24:22", "dateTime": "2024-06-04T16:24:22Z", "dateTimePub": "2024-06-04T16:23:33Z", "dataType": "news", "sim": 0.7843137383460999, "url": "https://wtop.com/news/2024/06/former-openai-employees-lead-push-to-protect-whistleblowers-flagging-artificial-intelligence-risks/", "title": "Former OpenAI employees lead push to protect whistleblowers flagging artificial intelligence risks - WTOP News", "body": "A group of OpenAI's current and former workers is calling on the ChatGPT-maker and other artificial intelligence companies to protect...\n\nA group of OpenAI's current and former workers is calling on the ChatGPT-maker and other artificial intelligence companies to protect whistleblowing employees who flag safety risks about AI technology.\n\nAn open letter published Tuesday asks tech companies to establish stronger whistleblower protections so researchers can raise concerns about the development of high-performing AI systems internally and with the public without fear of retaliation.\n\nFormer OpenAI employee Daniel Kokotajlo, who left the company earlier this year, said in a written statement that tech companies are \"disregarding the risks and impact of AI\" as they race to develop better-than-human AI systems known as artificial general intelligence.\n\n\"I decided to leave OpenAI because I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence,\" he wrote. \"They and others have bought into the 'move fast and break things' approach and that is the opposite of what is needed for technology this powerful and this poorly understood.\"\n\nOpenAI said in a statement responding to the letter that it already has measures for employees to express concerns, including an anonymous integrity hotline.\n\n\"We're proud of our track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk,\" said the company's statement. \"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"\n\nThe letter has 13 signatories, most of whom are former employees of OpenAI and two who work or worked for Google's DeepMind. Four are listed as anonymous current employees of OpenAI. The letter asks that companies stop making workers enter into \"non-disparagement\" agreements that can punish them if they criticize the company.\n\nSocial media outrage over language in OpenAI's paperwork for departing workers recently led the company to release all its former employees from their non-disparagement agreements.\n\nThe open letter has the support of pioneering AI scientists Yoshua Bengio and Geoffrey Hinton, who together won computer science's highest award, and Stuart Russell. All three have warned about the risks that future AI systems could pose to humanity's existence.\n\nThe letter comes as OpenAI has said it is beginning to develop the next generation of the AI technology behind ChatGPT and forming a new safety committee just after losing a set of leaders, including co-founder Ilya Sutskever, who were part of a team focused on safely developing the most powerful AI systems. The broader AI research community has long battled over the gravity of AI's short-term and long-term risks and how to square them with the technology's commercialization.\n\nThose conflicts contributed to the ouster, and swift return, of OpenAI CEO Sam Altman last year, and continue to fuel distrust in his leadership.\n\nMore recently, a new product showcase drew the ire of Hollywood star Scarlett Johansson, who said she was shocked to hear ChatGPT's voice sounding \"eerily similar\" to her own despite having previously rejected Altman's request that she lend her voice to the system.\n\n-- -- -\n\nThe Associated Press and OpenAI have a licensing and technology agreement that allows OpenAI access to part of AP's text archives.\n\nCopyright \u00a9 2024 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.", "source": {"uri": "wtop.com", "dataType": "news", "title": "WTOP"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 4, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 2, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 1, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 1, "label": {"eng": "Scarlett Johansson"}}, {"uri": "http://en.wikipedia.org/wiki/Associated_Press", "type": "org", "score": 1, "label": {"eng": "Associated Press"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Social_media", "type": "wiki", "score": 1, "label": {"eng": "Social media"}}], "categories": [{"uri": "dmoz/Computers/Artificial_Intelligence/Associations", "label": "dmoz/Computers/Artificial Intelligence/Associations", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 35}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 26}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 29}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 28}], "image": null, "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.4823529411764707, "wgt": 200, "relevance": 1}
{"uri": "8161530549", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:14:08", "dateTime": "2024-06-04T17:14:08Z", "dateTimePub": "2024-06-04T17:13:26Z", "dataType": "news", "sim": 0.7803921699523926, "url": "https://www.theguardian.com/technology/article/2024/jun/04/openai-google-ai-risks-letter", "title": "OpenAI and Google DeepMind workers warn of AI industry risks in open letter", "body": "Current and former workers sign letter warning of lack of safety oversight and calling for more protections for whistleblowers\n\nA group of current and former employees at prominent artificial intelligence companies issued an open letter on Tuesday that warned of a lack of safety oversight within the industry and called for increased protections for whistleblowers.\n\nThe letter, which calls for a \"right to warn about artificial intelligence\", is one of the most public statements about the dangers of AI from employees within what is generally a secretive industry. Eleven current and former OpenAI workers signed the letter, along with two current or former Google DeepMind employees - one of whom previously worked at Anthropic.\n\n\"AI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm,\" the letter states. \"However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nOpenAI defended its practices in a statement, saying that it has avenues such as a tipline to report issues at the company and that it does not release new technology until there are appropriate safeguards. Google did not immediately respond to a request for comment.\n\n\"We're proud of our track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk. We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world,\" an OpenAI spokesperson said.\n\nConcern over the potential harms of artificial intelligence have existed for decades, but the AI boom of recent years has intensified those fears and left regulators scrambling to catch up with technological advancements. While AI companies have publicly stated their commitment to safely developing the technology, researchers and employees have warned about a lack of oversight as AI tools exacerbate existing social harms or create entirely new ones.\n\nThe letter from current and former AI company employees, which was first reported by the New York Times, calls for increased protections for workers at advanced AI companies who decide to voice safety concerns. It asks for a commitment to four principles around transparency and accountability, including a provision that companies will not force employees to sign any non-disparagement agreements that prohibit airing risk-related AI issues and a mechanism for employees to anonymously share concerns with board members.\n\n\"So long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public,\" the letter states. \"Yet broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues.\"\n\nCompanies such as OpenAI have also pursued aggressive tactics to prevent employees from speaking freely about their work, with Vox reporting last week that OpenAI made employees who leave the company sign extremely restrictive nondisparagement and nondisclosure documents or lose all their vested equity. Sam Altman, OpenAI's CEO, apologized following the report, saying that he would change off-boarding procedures.\n\nThe letter comes after two top OpenAI employees, co-founder Ilya Sutskever and key safety researcher Jan Leike, resigned from the company last month. After his departure, Leike alleged that OpenAI had abandoned a culture of safety in favor of \"shiny products\".\n\nThe open letter on Tuesday echoed some of Leike's statement, saying that companies did not display any obligation to be transparent about their operations.", "source": {"uri": "theguardian.com", "dataType": "news", "title": "The Guardian"}, "authors": [{"uri": "nick_robins_early@theguardian.com", "name": "Nick Robins-Early", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 5, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 3, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 2, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Vox_(website)", "type": "wiki", "score": 1, "label": {"eng": "Vox (website)"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 1, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Transparency_(behavior)", "type": "wiki", "score": 1, "label": {"eng": "Transparency (behavior)"}}, {"uri": "http://en.wikipedia.org/wiki/Accountability", "type": "wiki", "score": 1, "label": {"eng": "Accountability"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 1, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 1, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 1, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Society/Work", "label": "dmoz/Society/Work", "wgt": 20}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 24}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 20}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 59}], "image": "https://i.guim.co.uk/img/media/bda61d4c1f0fa5044f797e27952705dd1f086763/0_180_5372_3222/master/5372.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&s=38c8353989c6f4dc904b28b308ffffa0", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.1450980392156862, "wgt": 199, "relevance": 1}
{"uri": "2024-06-378677857", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:46:32", "dateTime": "2024-06-04T16:46:32Z", "dateTimePub": "2024-06-04T16:46:11Z", "dataType": "news", "sim": 0.772549033164978, "url": "https://venturebeat.com/ai/more-openai-researchers-slam-company-on-safety-call-for-right-to-warn-to-avert-human-extinction/", "title": "More OpenAI researchers slam company on safety, call for 'right to warn' to avert 'human extinction'", "body": "Time's almost up! There's only one week left to request an invite to The AI Impact Tour on June 5th. Don't miss out on this incredible opportunity to explore various methods for auditing AI models. Find out how you can attend here.\n\nA group of 11 researchers who currently or formerly worked at OpenAI, as well as a current member of Google DeepMind who previously worked at Anthropic, and another former DeepMind researcher, have signed a new open letter online calling for OpenAI and similar companies to commit to four principles protecting whistleblowers and critics who raise issues surrounding AI safety.\n\n\"We also understand the serious risks posed by these technologies,\" the letter, titled \"Right to Warn,\" states: \"These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction.\"\n\nWhat is a 'Right to Warn' for AI systems?\n\nAmong the concerns expressed in the letter are the lack of proper oversight, the influence of profit motives, and the suppression of dissenting voices within organizations working on cutting-edge AI technologies.\n\nThe four principles the signatories want AI companies to voluntarily agree to abide by to rectify these are as follows:\n\nThe letter, which was first publicized in an article published today in The New York Times, is signed by former OpenAI employees Jacob Hilton, Daniel Kokotajlo, William Saunders, and Daniel Ziegler, former Google DeepMinder Ramana Kumar, and current DeeMinder and former Anthropic AI employee Neel Nanda, as well as six anonymous former OpenAI members. It is endorsed by notable AI experts Yoshua Bengio, Geoffrey Hinton, and Stuart Russell.\n\nFurthermore, in a series of posts on X (formerly Twitter) following the NYT article, Kokotajlo elaborated on his reasons for resigning from OpenAI, claiming that he lost confidence in the company's ability to act responsibly in its pursuit of artificial general intelligence.\n\nHe revealed that he chose to give up his vested equity in order to speak critically about the company, highlighting the need for transparency and ethical conduct in the development of advanced AI systems.\n\nAccording to Kokotajlo, he joined OpenAI hoping that the company would increase investment in safety research as its systems became more capable.\n\nHowever, he states that OpenAI failed to make this pivot, prompting several researchers, including himself, to leave the company.\n\nKokotajlo alleges that upon his departure, he was presented with paperwork containing a non-disparagement agreement (NDA) intended to prevent him from speaking negatively about OpenAI, which he deemed unethical.\n\nThese claims follow revelations of similar practices within OpenAI earlier this month, where leaked documents exposed by Vox showed the use of strong-arm tactics towards former employees.\n\nYet OpenAI has said it won't enforce these NDAs -- some of which are used by other tech companies in AI and beyond. And Vox itself recently elected to partner with OpenAI following its own reporting on the company.\n\nOngoing period of turbulence for OpenAI\n\nThis wave of criticism directed at OpenAI follows a long and ongoing period of turbulence for the company that began in November 2023 when the former non-profit board that oversaw the company abruptly fired OpenAI co-founder and CEO Sam Altman for alleged \"not consistently candid\" communications with them.\n\nAltman was rapidly reinstated as CEO at the behest of investors including Microsoft, and the former board resigned and was replaced, but one member, Helen Toner, reiterated her concerns in an interview on the TED AI Show last week, saying the board was not informed prior to the public release of ChatGPT in November 2022.\n\nAnd following OpenAI's release of the new GPT-4o natively multimodal AI model in mid-May, celebrity actor Scarlett Johansson sharply criticized the company and Altman for soliciting her to voice its new conversational interface, only for her to decline, while OpenAI showcased a demo voice she thought sounded like her AI operating system character from the 2013 sci-fi drama film Her.\n\nYet a subsequent report in the Washington Post bolstered OpenAI's claim that it recorded the voice from a separate voice actor without any intention of it sounding like Johansson's character.\n\nAdditional independent research has shown the OpenAI voice, \"Sky,\" more closely resembles other Hollywood actors such as Keri Russell, though it it remains distinct from her as well. OpenAI has since removed this voice, named \"Sky\" out of an apparent desire to avoid confusion and appease Johansson.\n\nAdditionally, the departures of high-profile figures involved in AI safety efforts, namely former superalignment team co-leaders Ilya Sutskever and Jan Leike, have further fueled concerns regarding OpenAI's safety policies and practices.\n\nThe company has attempted to meet these concerns on its own terms with the formation of a new Safety and Security Committee including many of its current board members, which was announced last week to the day alongside the news that OpenAI has begun training its latest frontier model.\n\nFull \"Right to Warn\" letter text:\n\nA Right to Warn about Advanced Artificial Intelligence\n\nWe are current and former employees at frontier AI companies, and we believe in the potential of AI technology to deliver unprecedented benefits to humanity.\n\nWe also understand the serious risks posed by these technologies. These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction. AI companies themselves have acknowledged these risks [1, 2, 3], as have governments across the world [4, 5, 6] and other AI experts [7, 8, 9].\n\nWe are hopeful that these risks can be adequately mitigated with sufficient guidance from the scientific community, policymakers, and the public. However, AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this.\n\nAI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm. However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\n\nSo long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public. Yet broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues. Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated. Some of us reasonably fear various forms of retaliation, given the history of such cases across the industry. We are not the first to encounter or speak about these issues.", "source": {"uri": "venturebeat.com", "dataType": "news", "title": "VentureBeat"}, "authors": [{"uri": "carl_franzen@venturebeat.com", "name": "Carl Franzen", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 5, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 5, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 4, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 4, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 3, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Vox_(website)", "type": "wiki", "score": 3, "label": {"eng": "Vox (website)"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 3, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 3, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 3, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Audit", "type": "wiki", "score": 3, "label": {"eng": "Audit"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 3, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 3, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 2, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Multimodal_learning", "type": "wiki", "score": 2, "label": {"eng": "Multimodal learning"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 2, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/TED_(conference)", "type": "wiki", "score": 2, "label": {"eng": "TED (conference)"}}, {"uri": "http://en.wikipedia.org/wiki/Transparency_(behavior)", "type": "wiki", "score": 2, "label": {"eng": "Transparency (behavior)"}}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 2, "label": {"eng": "Scarlett Johansson"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 2, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Nonprofit_organization", "type": "wiki", "score": 2, "label": {"eng": "Nonprofit organization"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 2, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Twitter", "type": "org", "score": 2, "label": {"eng": "Twitter"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Cinema_of_the_United_States", "type": "loc", "score": 1, "label": {"eng": "Cinema of the United States"}, "location": null}, {"uri": "http://en.wikipedia.org/wiki/The_Washington_Post", "type": "org", "score": 1, "label": {"eng": "The Washington Post"}}], "categories": [{"uri": "dmoz/Society/Issues/Business", "label": "dmoz/Society/Issues/Business", "wgt": 18}, {"uri": "dmoz/Arts/Animation/Voice_Actors", "label": "dmoz/Arts/Animation/Voice Actors", "wgt": 19}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 27}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 23}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 84}], "image": "https://venturebeat.com/wp-content/uploads/2024/06/cfr0z3n_line_art_vector_art_colorful_graphical_a_crowd_of_young_42739acc-c3fb-493e-99f8-e7b740da70dd.png?w=1024?w=1200&strip=all", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": [{"amb": false, "imp": true, "date": "2024-06-05", "textStart": 91, "textEnd": 99}], "sentiment": 0.003921568627450966, "wgt": 197, "relevance": 1}
{"uri": "2024-06-378672442", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:41:03", "dateTime": "2024-06-04T16:41:03Z", "dateTimePub": "2024-06-04T12:19:00Z", "dataType": "news", "sim": 0.772549033164978, "url": "https://www.usnews.com/news/best-states/california/articles/2024-06-04/former-openai-employees-lead-push-to-protect-whistleblowers-flagging-artificial-intelligence-risks", "title": "Former OpenAI Employees Lead Push to Protect Whistleblowers Flagging Artificial Intelligence Risks", "body": "A group of OpenAI's current and former workers is calling on the ChatGPT-maker and other artificial intelligence companies to protect whistleblowing employees who flag safety risks about AI technology.\n\nAn open letter published Tuesday asks tech companies to establish stronger whistleblower protections so researchers can raise concerns about the development of high-performing AI systems internally and with the public without fear of retaliation.\n\nFormer OpenAI employee Daniel Kokotajlo, who left the company earlier this year, said in a written statement that tech companies are \"disregarding the risks and impact of AI\" as they race to develop better-than-human AI systems known as artificial general intelligence.\n\n\"I decided to leave OpenAI because I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence,\" he wrote. \"They and others have bought into the 'move fast and break things' approach and that is the opposite of what is needed for technology this powerful and this poorly understood.\"\n\nOpenAI said in a statement responding to the letter that it already has measures for employees to express concerns, including an anonymous integrity hotline.\n\n\"We're proud of our track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk,\" said the company's statement. \"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"\n\nThe letter has 13 signatories, most of whom are former employees of OpenAI and two who work or worked for Google's DeepMind. Four are listed as anonymous current employees of OpenAI. The letter asks that companies stop making workers enter into \"non-disparagement\" agreements that can punish them if they criticize the company.\n\nSocial media outrage over language in OpenAI's paperwork for departing workers recently led the company to release all its former employees from their non-disparagement agreements.\n\nThe open letter has the support of pioneering AI scientists Yoshua Bengio and Geoffrey Hinton, who together won computer science's highest award, and Stuart Russell. All three have warned about the risks that future AI systems could pose to humanity's existence.\n\nThe letter comes as OpenAI has said it is beginning to develop the next generation of the AI technology behind ChatGPT and forming a new safety committee just after losing a set of leaders, including co-founder Ilya Sutskever, who were part of a team focused on safely developing the most powerful AI systems. The broader AI research community has long battled over the gravity of AI's short-term and long-term risks and how to square them with the technology's commercialization.\n\nThose conflicts contributed to the ouster, and swift return, of OpenAI CEO Sam Altman last year, and continue to fuel distrust in his leadership.\n\nMore recently, a new product showcase drew the ire of Hollywood star Scarlett Johansson, who said she was shocked to hear ChatGPT's voice sounding \"eerily similar\" to her own despite having previously rejected Altman's request that she lend her voice to the system.\n\n-- -- -\n\nThe Associated Press and OpenAI have a licensing and technology agreement that allows OpenAI access to part of AP's text archives.\n\nCopyright 2024 The Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.", "source": {"uri": "usnews.com", "dataType": "news", "title": "U.S. News & World Report"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 4, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 2, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 1, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 1, "label": {"eng": "Scarlett Johansson"}}, {"uri": "http://en.wikipedia.org/wiki/Associated_Press", "type": "org", "score": 1, "label": {"eng": "Associated Press"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Social_media", "type": "wiki", "score": 1, "label": {"eng": "Social media"}}], "categories": [{"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 22}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 33}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 23}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 28}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 27}], "image": null, "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.419607843137255, "wgt": 197, "relevance": 1}
{"uri": "8161371909", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:22:43", "dateTime": "2024-06-04T15:22:43Z", "dateTimePub": "2024-06-04T15:21:17Z", "dataType": "news", "sim": 0.772549033164978, "url": "https://www.yahoo.com/tech/openai-employees-demanding-change-4-143133800.html", "title": "OpenAI employees are demanding change. Here are the 4 things they want.", "body": "Their open letter, which has 4 demands, was endorsed by the \"Godfathers of AI.\"\n\nA group of nine current and former OpenAI employees signed a letter calling out tech firms over major concerns about the risks of artificial intelligence.\n\nIn their letter, the tech workers called for more transparency in AI companies and better protections for whistleblowers who wish to raise concerns about the power of AI.\n\n\"We are current and former employees at frontier AI companies, and we believe in the potential of AI technology to deliver unprecedented benefits to humanity,\" the letter said.\n\n\"We also understand the serious risks posed by these technologies. These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction,\" it continued. \"AI companies themselves have acknowledged these risks, as have governments across the world, and other AI experts.\"\n\nA total of 13 people signed the letter, and they all come from some of the top players in AI -- including OpenAI, Anthropic, and DeepMind. It was also endorsed by two men known as the \"Godfathers of AI,\" Yoshua Bengio and Geoffrey Hinton.\n\n\"I decided to leave OpenAI because I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence,\" former OpenAI employee Daniel Kokotajlo said in a statement.\n\n\"They and others have bought into the 'move fast and break things' approach and that is the opposite of what is needed for technology this powerful and this poorly understood,\" he added.\n\nThe AI employees outlined a list of four demands that they said would help mitigate the existing issues of inequality and misinformation in the AI space.\n\nHere's a look at the four principles the 13 employees said they want OpenAI and other AI companies to adopt, according to the letter.\n\nAn OpenAI spokesperson told Business Insider that the company is \"proud of our track record providing the most capable and safest A.I. systems and believe in our scientific approach to addressing risk.\"\n\n\"We agree that rigorous debate is crucial given the significance of this technology, and we'll continue to engage with governments, civil society and other communities around the world,\" the statement continued.\n\nSpokespeople for Google Deepmind and Anthropic did not immediately respond to BI's request for comment ahead of publication.", "source": {"uri": "yahoo.com", "dataType": "news", "title": "Yahoo"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 2, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 2, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Business_Insider", "type": "wiki", "score": 1, "label": {"eng": "Business Insider"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 1, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Economic_inequality", "type": "wiki", "score": 1, "label": {"eng": "Economic inequality"}}], "categories": [{"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 17}, {"uri": "dmoz/Computers/Artificial_Intelligence/Associations", "label": "dmoz/Computers/Artificial Intelligence/Associations", "wgt": 15}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 28}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 22}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 52}], "image": "https://s.yimg.com/ny/api/res/1.2/4oD1jo95p9OQuWUhOoXuZg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/business_insider_articles_888/c0677920c8c630d1ed3811280d7e9bc3", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.07450980392156858, "wgt": 197, "relevance": 1}
{"uri": "8161550790", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:31:13", "dateTime": "2024-06-04T17:31:13Z", "dateTimePub": "2024-06-04T17:29:46Z", "dataType": "news", "sim": 0.7647058963775635, "url": "https://www.reformer.com/news/national/openai-insiders-blast-lack-of-ai-transparency/article_0514ba62-89e6-54d5-ad8d-2adc81b93a7f.html", "title": "OpenAI insiders blast lack of AI transparency", "body": "A group of current and former employees from OpenAI on Tuesday issued an open letter warning that the world's leading artificial intelligence companies were falling short of necessary transparency and accountability to meet the potential risks posed by the technology.\n\nThe letter raised serious concerns about AI safety risks \"ranging from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction.\"\n\nThe 16 signatories, which also included a staff member from Google DeepMind, warned that AI companies \"have strong financial incentives to avoid effective oversight\" and that self-regulation by the companies would not effectively change this.\n\n\"AI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm,\" the letter said.\n\n\"However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThat reality, the letter added, meant that employees inside the companies were the only ones who could notify the public, and the signatories called for broader whistleblower laws to protect them.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the letter said.\n\nThe four current employees of OpenAI signed the letter anonymously because they feared retaliation from the company, The New York Times reported.\n\nIt was also signed by Yoshua Bengio, Geoffrey Hinton and Stuart Russell, who are often described as AI \"godfathers\" and have criticized the lack of preparation for AI's dangers.\n\nMore from this section Germany: Catastrophic Flooding Hits Southern Region 2 Eamonn Holmes thanks Kerry Katona after message of support: 'I love you!' 'Trump Will Be Loser President If...'- Zelensky Mocks Biden's Republican Challenger\n\nOpenAI in a statement pushed back at the criticism.\n\n\"We're proud of our track record of providing the most capable and safest AI systems and believe in our scientific approach to addressing risk,\" a statement said.\n\n\"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"\n\nOpenAI also said it had \"avenues for employees to express their concerns including an anonymous integrity hotline\" and a newly formed Safety and Security Committee led by members of the board and executives, including CEO Sam Altman.\n\nThe criticism of OpenAI, which was first released to the Times, comes as questions are growing around Altman's leadership of the company.\n\nOpenAI has unveiled a wave of new products, though the company insists they will only get released to the public after thorough testing.\n\nAn unveiling of a human-like chatbot caused a controversy when Hollywood star Scarlett Johansson complained that it closely resembled her voice.\n\nShe had previously turned down an offer from Altman to work with the company.\n\narp/sst", "source": {"uri": "reformer.com", "dataType": "news", "title": "Brattleboro Reformer"}, "authors": [{"uri": "agence_france_presse@reformer.com", "name": "Agence France-Presse", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 4, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 3, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Accountability", "type": "wiki", "score": 3, "label": {"eng": "Accountability"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 2, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 2, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 2, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 1, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Kerry_Katona", "type": "person", "score": 1, "label": {"eng": "Kerry Katona"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Eamonn_Holmes", "type": "person", "score": 1, "label": {"eng": "Eamonn Holmes"}}, {"uri": "http://en.wikipedia.org/wiki/Cinema_of_the_United_States", "type": "loc", "score": 1, "label": {"eng": "Cinema of the United States"}, "location": null}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 1, "label": {"eng": "Scarlett Johansson"}}, {"uri": "http://en.wikipedia.org/wiki/Republican_Party_(United_States)", "type": "org", "score": 1, "label": {"eng": "Republican Party (United States)"}}, {"uri": "http://en.wikipedia.org/wiki/Donald_Trump", "type": "person", "score": 1, "label": {"eng": "Donald Trump"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Germany", "type": "loc", "score": 1, "label": {"eng": "Germany"}, "location": {"type": "country", "label": {"eng": "Germany"}}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 16}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 16}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 19}, {"uri": "news/Arts_and_Entertainment", "label": "news/Arts and Entertainment", "wgt": 66}], "image": "https://bloximages.newyork1.vip.townnews.com/reformer.com/content/tncms/assets/v3/editorial/3/e0/3e0c1c0d-6ea2-591a-86a8-fc0cce43dd5d/664d1deb965a4.image.jpg?crop=512%2C269%2C0%2C36&resize=438%2C230&order=crop%2Cresize", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.08235294117647063, "wgt": 195, "relevance": 1}
{"uri": "8161407291", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:46:31", "dateTime": "2024-06-04T15:46:31Z", "dateTimePub": "2024-06-04T15:45:17Z", "dataType": "news", "sim": 0.7607843279838562, "url": "https://nypost.com/2024/06/04/business/openai-google-ignoring-risks-in-race-for-advanced-ai-should-allow-right-to-warn-public-employees/", "title": "Employees claim OpenAI, Google ignoring risks of AI  --  and should...", "body": "A group of AI whistleblowers claim tech giants like Google and ChatGPT creator OpenAI are locked in a reckless race to develop technology that could endanger humanity - and demanded \"a right to warn\" the public in an open letter Tuesday.\n\nSigned by current and former employees of OpenAI, Google DeepMind and Anthropic, the open letter cautioned that \"AI companies have strong financial incentives to avoid effective oversight\" and cited a lack of federal rules on developing advanced AI.\n\nThe workers point to potential risks including the spread of misinformation, worsening inequality and even \"loss of control of autonomous AI systems potentially resulting in human extinction\" - especially as OpenAI and other firms pursue so-called advanced general intelligence, with capacities on par with or surpassing the human mind.\n\n\"Companies are racing to develop and deploy ever more powerful artificial intelligence, disregarding the risks and impact of AI,\" former OpenAI employee Daniel Kokotajlo, one of the letter's organizers, said in a statement. \"I decided to leave OpenAI because I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence.\n\n\"They and others have bought into the 'move fast and break things' approach and that is the opposite of what is needed for technology this powerful and this poorly understood,\" Kokotajlo added.\n\nThe letter drew endorsements by two prominent experts known as the \"Godfathers of AI\" -- Geoffrey Hinton, who warned last year that the threat of rogue AI was \"more urgent\" to humanity than climate change, and Canadian computer scientist Yoshua Bengio. Famed British AI researcher Stuart Russell also backed the letter.\n\nThe letter asks AI giants to commit to four principles designed to boost transparency and protect whistleblowers who speak out publicly.\n\nThose include an agreement not to retaliate against employees who speak out about safety concerns and to support an anonymous system for whistleblowers to alert the public and regulators about risks.\n\nThe AI firms are also asked to allow a \"culture of open criticism\" so long as no trade secrets are disclosed, and pledge not to enter into or enforce non-disparagement agreements or non-disclosure agreements.\n\nAs of Tuesday morning, the letter's signers include a total of 13 AI workers. Of that total, 11 are formerly or currently employed by OpenAI, including Kokotajlo, Jacob Hilton, William Saunders, Carroll Wainwright and Daniel Ziegler.\n\n\"There should be ways to share information about risks with independent experts, governments, and the public,\" said Saunders. \"Today, the people with the most knowledge about how frontier AI systems work and the risks related to their deployment are not fully free to speak because of possible retaliation and overly broad confidentiality agreements.\"\n\nOther signers included former Google DeepMind employee Ramana Kumar and current employee Neel Nanda, who formerly worked at Anthropic.\n\nWhen reached for comment, an OpenAI spokesperson said the company has a proven track record of not releasing AI products until necessary safeguards were in place.\n\n\"We're proud of our track record providing the most capable and safest A.I. systems and believe in our scientific approach to addressing risk,\" OpenAI said in a statement.\n\n\"We agree that rigorous debate is crucial given the significance of this technology, and we'll continue to engage with governments, civil society and other communities around the world,\" the company added\n\nGoogle and Anthropic did not immediately return requests for comment.\n\nThe letter was published just days after revelations that OpenAI has dissolved its \"Superalignment\" safety team, whose responsibilities included creating safety measures for advanced general intelligence (AGI) systems that \"could lead to the disempowerment of humanity or even human extinction.\"\n\nTwo OpenAI executives who led the team, co-founder Ilya Sutskever and Jan Leike, have since resigned from the company. Leike blasted the firm on his way out the door, claiming that safety had \"taken a backseat to shiny products.\"\n\nElsewhere, former OpenAI board member Helen Toner - who was part of the group that briefly succeeded in ousting Sam Altman as the firm's CEO last year - alleged that he had repeatedly lied during her tenure.\n\nToner claimed that she and other board members did not learn about ChatGPT's launch in November 2022 from Altman and instead found out about its debut on Twitter.\n\nOpenAI has since established a new safety oversight committee that includes Altman as it begins training the new version of the AI model that powers ChatGPT.\n\nThe company pushed back on Toner's allegations, noting that an outside review had determined that safety concerns were not a factor in Altman's removal.", "source": {"uri": "nypost.com", "dataType": "news", "title": "New York Post"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 5, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 4, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 4, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 3, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Economic_inequality", "type": "wiki", "score": 3, "label": {"eng": "Economic inequality"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 3, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 2, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Computer_scientist", "type": "wiki", "score": 2, "label": {"eng": "Computer scientist"}}, {"uri": "http://en.wikipedia.org/wiki/Climate_change", "type": "wiki", "score": 2, "label": {"eng": "Climate change"}}, {"uri": "http://en.wikipedia.org/wiki/United_Kingdom", "type": "loc", "score": 2, "label": {"eng": "United Kingdom"}, "location": {"type": "country", "label": {"eng": "United Kingdom"}}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Twitter", "type": "org", "score": 1, "label": {"eng": "Twitter"}}], "categories": [{"uri": "dmoz/Computers/Artificial_Intelligence", "label": "dmoz/Computers/Artificial Intelligence", "wgt": 18}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 26}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 23}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 48}], "image": "https://nypost.com/wp-content/uploads/sites/2/2024/06/83238295.jpg?quality=75&strip=all&w=1024", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.2078431372549019, "wgt": 194, "relevance": 1}
{"uri": "8161399691", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:41:11", "dateTime": "2024-06-04T15:41:11Z", "dateTimePub": "2024-06-04T15:40:13Z", "dataType": "news", "sim": 0.7607843279838562, "url": "https://www.bnnbloomberg.ca/openai-employees-want-protections-to-speak-out-on-serious-risks-of-ai-1.2080994", "title": "OpenAI Employees Want Protections to Speak Out on 'Serious Risks' of AI -  BNN Bloomberg", "body": "(Bloomberg) -- A group of current and former employees from OpenAI and Google DeepMind have signed an open letter asking for protection from retaliation for sharing concerns about the \"serious risks\" of the technologies these and other companies are building.\"So long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public,\" according to the letter, which was signed by 13 people who've worked at the companies, seven of whom included their names. \"Yet broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues.\"In recent weeks, OpenAI has faced controversy about its approach to safeguarding artificial intelligence after dissolving one of its most high-profile safety teams and being hit by a series of staff departures. OpenAI employees have also raised concerns that staffers were asked to sign nondisparagement agreements tied to their shares in the company, potentially causing them to lose out on lucrative equity deals if they speak out against the AI startup. After some pushback, OpenAI said it would release past employees from the agreements. Jacob Hilton, one of the former OpenAI employees who signed the letter Tuesday, wrote on X that the company deserves credit for the nondisparagement policy change, \"but employees may still fear other forms of retaliation for disclosure, such as being fired and sued for damages.\"In a statement sent to Bloomberg, a spokesperson for OpenAI said the company is proud of its \"track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk.\" The spokesperson added: \"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"A representative for Google did not immediately respond to a request for comment.In the letter, which was titled A Right to Warn about Advanced Artificial Intelligence, the employees said they're worried because leading AI companies \"have strong financial incentives to avoid effective oversight.\" On the other hand, the companies have \"only weak obligations\" to share the true dangers of their AI systems with the public, they said.The letter argued that ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks employees are concerned about are not yet regulated.", "source": {"uri": "bnnbloomberg.ca", "dataType": "news", "title": "BNN"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Bloomberg_News", "type": "org", "score": 4, "label": {"eng": "Bloomberg News"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 4, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 3, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/Startup_company", "type": "wiki", "score": 2, "label": {"eng": "Startup company"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 2, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 1, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 1, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 1, "label": {"eng": "Google"}}], "categories": [{"uri": "dmoz/Business/Business_Services/Signage", "label": "dmoz/Business/Business Services/Signage", "wgt": 19}, {"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 20}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 20}, {"uri": "dmoz/Society/Work/Coworker_Relations", "label": "dmoz/Society/Work/Coworker Relations", "wgt": 19}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 30}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 63}], "image": "http://www.bnnbloomberg.ca/polopoly_fs/1.2080995!/fileimage/httpImage/image.jpg_gen/derivatives/landscape_620/p-sam-altman-chief-executive-officer-of-openai-speaks-at-the-microsoft-build-event-in-seattle-washington-us-on-tuesday-may-21-2024-nbsp-p.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.05882352941176472, "wgt": 194, "relevance": 1}
{"uri": "8161520463", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:07:00", "dateTime": "2024-06-04T17:07:00Z", "dateTimePub": "2024-06-04T17:06:07Z", "dataType": "news", "sim": 0.7607843279838562, "url": "https://www.spacedaily.com/afp/240604162647.864vuw60.html", "title": "OpenAI insiders blast lack of AI transparency", "body": "The letter raised serious concerns about AI safety risks \"ranging from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction.\"\n\nThe 16 signatories, which also included a staff member from Google DeepMind, warned that AI companies \"have strong financial incentives to avoid effective oversight\" and that self-regulation by the companies would not effectively change this.\n\n\"AI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm,\" the letter said.\n\n\"However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThat reality, the letter added, meant that employees inside the companies were the only ones who could notify the public, and the signatories called for broader whistleblower laws to protect them.\n\n\"Broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues,\" the letter said.\n\nThe four current employees of OpenAI signed the letter anonymously because they feared retaliation from the company, The New York Times reported.\n\nIt was also signed by Yoshua Bengio, Geoffrey Hinton and Stuart Russell, who are often described as AI \"godfathers\" and have criticized the lack of preparation for AI's dangers.\n\nOpenAI in a statement pushed back at the criticism.\n\n\"We're proud of our track record of providing the most capable and safest AI systems and believe in our scientific approach to addressing risk,\" a statement said.\n\n\"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"\n\nOpenAI also said it had \"avenues for employees to express their concerns including an anonymous integrity hotline\" and a newly formed Safety and Security Committee led by members of the board and executives, including CEO Sam Altman.\n\nThe criticism of OpenAI, which was first released to the Times, comes as questions are growing around Altman's leadership of the company.\n\nOpenAI has unveiled a wave of new products, though the company insists they will only get released to the public after thorough testing.\n\nAn unveiling of a human-like chatbot caused a controversy when Hollywood star Scarlett Johansson complained that it closely resembled her voice.\n\nShe had previously turned down an offer from Altman to work with the company.", "source": {"uri": "spacedaily.com", "dataType": "news", "title": "SpaceDaily"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 3, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 3, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 2, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 2, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/Chatbot", "type": "wiki", "score": 1, "label": {"eng": "Chatbot"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Scarlett_Johansson", "type": "person", "score": 1, "label": {"eng": "Scarlett Johansson"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 14}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 16}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 16}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 15}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 18}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 48}], "image": null, "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.0980392156862745, "wgt": 194, "relevance": 1}
{"uri": "2024-06-378629361", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:58:52", "dateTime": "2024-06-04T15:58:52Z", "dateTimePub": "2024-06-04T15:51:44Z", "dataType": "news", "sim": 0.7568627595901489, "url": "https://eu.detroitnews.com/story/business/2024/06/04/current-and-former-ai-employees-warn-of-the-technologys-dangers/73970975007/", "title": "Current and former AI employees warn of the technology's dangers", "body": "A handful of current and former employees at prominent artificial intelligence companies warned the technology poses grave risks to humanity in a Tuesday letter, calling on corporations to commit to being more transparent and fostering a culture of criticism that holds them more accountable.\n\nThe letter, signed by 13 people, including current and former employees at OpenAI, Anthropic and Google's DeepMind, said AI can exacerbate inequality, increase misinformation and allow AI systems to become autonomous and cause significant death. Though these risks could be mitigated, corporations in control of the software have \"strong financial incentives\" to limit oversight, they said.\n\nThe move comes as OpenAI faces a staff exodus. Many critics have seen prominent departures - including OpenAI co-founder Ilya Sutskever and senior researcher Jan Leike - as a rebuke of company leaders, who some employees argue chase profit at the expense of making OpenAI's technologies safer.\n\nDaniel Kokotajlo, a former employee at OpenAI, said he left the start-up because of the company's disregard for the risks of artificial intelligence.\n\n\"I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence,\" he said in a statement, referencing a hotly contested term referring to computers matching the power of human brains.\n\n\"They and others have bought into the 'move fast and break things' approach, and that is the opposite of what is needed for technology this powerful and this poorly understood.\"\n\nLiz Bourgeois, a spokesperson at OpenAI, said the company agrees that \"rigorous debate is crucial given the significance of this technology.\" Representatives from Anthropic and Google did not immediately reply to a request for comment.\n\nThe employees said that absent government oversight, AI workers are the \"few people\" who can hold corporations accountable. They noted that they are hamstrung by \"broad confidentiality agreements\" and that ordinary whistleblower protections are \"insufficient\" because they focus on illegal activity, and the risks that they are warning about are not yet regulated.\n\nThe letter called for AI companies to commit to four principles to allow for greater transparency and whistleblower protections. Those principles include a commitment to not enter into or enforce agreements that prohibit criticism of risks; a call to establish an anonymous process for current and former employees to raise concerns; supporting a culture of criticism; and a promise to not retaliate against current and former employees who share confidential information to raise alarms \"after other processes have failed.\"\n\nThe Washington Post in December reported that senior leaders at OpenAI raised fears about retaliation from CEO Sam Altman - warnings that preceded the chief's temporary ousting. In a recent podcast interview, former OpenAI board member Helen Toner said part of the nonprofit's decision to remove Altman as CEO late last year was his lack of candid communication about safety.\n\n\"He gave us inaccurate information about the small number of formal safety processes that the company did have in place, meaning that it was basically just impossible for the board to know how well those safety processes were working,\" she told \"The TED AI Show\" in May.\n\nThe letter was endorsed by AI luminaries including Yoshua Bengio and Geoffrey Hinton, who are considered \"godfathers\" of AI, and renowned computer scientist Stuart Russell.", "source": {"uri": "detroitnews.com", "dataType": "news", "title": "The Detroit News"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 5, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 4, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 3, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Software", "type": "wiki", "score": 3, "label": {"eng": "Software"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Human_brain", "type": "wiki", "score": 2, "label": {"eng": "Human brain"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Computer", "type": "wiki", "score": 2, "label": {"eng": "Computer"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 2, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 1, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 1, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/TED_(conference)", "type": "wiki", "score": 1, "label": {"eng": "TED (conference)"}}, {"uri": "http://en.wikipedia.org/wiki/Computer_scientist", "type": "wiki", "score": 1, "label": {"eng": "Computer scientist"}}, {"uri": "http://en.wikipedia.org/wiki/The_Washington_Post", "type": "org", "score": 1, "label": {"eng": "The Washington Post"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 19}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 23}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 23}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 20}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 47}], "image": "https://www.gannett-cdn.com/authoring/authoring-images/2024/06/04/PDTN/73970946007-ailetter-4-cvdcezvysog-5-dkydv-57-l-355-ri.jpg?auto=webp&crop=3311,1863,x442,y879&format=pjpg&width=1200", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.0117647058823529, "wgt": 193, "relevance": 1}
{"uri": "2024-06-378655207", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:23:08", "dateTime": "2024-06-04T16:23:08Z", "dateTimePub": "2024-06-04T16:17:24Z", "dataType": "news", "sim": 0.7450980544090271, "url": "https://www.cnn.com/2024/06/04/tech/openai-insiders-letter/index.html", "title": "OpenAI insiders call for company to be more transparent about the 'serious risks' AI technology poises to society", "body": "Current and former OpenAI employees are speaking out about the need for the company and others like it to be more transparent about the technology they're developing\n\nA group of OpenAI insiders are demanding that artificial intelligence companies be far more transparent about AI's \"serious risks\" -- and that they protect employees who voice concerns about the technology they're building.\n\n\"AI companies have strong financial incentives to avoid effective oversight,\" reads the open letter posted Tuesday signed by current and former employees at AI companies including OpenAI, the creator behind the viral ChatGPT tool.\n\nThey also called for AI companies to foster \"a culture of open criticism\" that welcomes, rather than punishes, people who speak up about their concerns, especially as the law struggles to catch up to the quickly advancing technology.\n\nCompanies have acknowledged the \"serious risks\" posed by AI -- from manipulation to a loss of control, known as \"singularity, that could potentially result in human extinction -- but they should be be doing more to educate the public about risks and protective measures, the group wrote.\n\nAs the law currently stands, the AI employees said, they don't believe AI companies will share critical information about the technology voluntarily.\n\nIt's essential, then, for current and former employees to speak up -- and for companies not to enforce \"disparagement\" agreements or otherwise retaliate against those who voice risk-related concerns. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated,\" the group wrote.\n\nTheir letter comes as companies move quickly to implement generative AI tools into their products, while government regulators, companies and consumers grapple with responsible use. Meanwhile many tech experts, researchers and leaders have called for a temporary pause in the AI race, or for the government to step in and create a moratorium.\n\nOpenAI's response\n\nIn response to the letter, OpenAI spokesperson told CNN it is \"proud of our track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk, adding that the company agrees \"rigorous debate is crucial given the significance of this technology.\"\n\nOpenAI noted it has an anonymous integrity hotline and a Safety and Security Committee led by members of its board and safety leaders from the company. The company does not sell personal info, build user profiles, or use that data to target anyone or sell anything.\n\nBut Daniel Ziegler, one of the organizers behind the letter and an early machine-learning engineer who worked at OpenAI between 2018 and 2021, told CNN that it's important to remain skeptical of the company's commitment to transparency.\n\n\"It's really hard to tell from the outside how seriously they're taking their commitments for safety elevations and figuring out societal harms, especially as there is such strong commercial pressures to move very quickly,\" he said. \"It's really important to have the right culture and processes so that employees can speak out in targeted ways when they have concerns.\"\n\nHe hopes more professionals in the AI industry will go public with their concerns as a result of the letter.\n\nMeanwhile, Apple is widely expected to announce a partnership with OpenAI at its annual Worldwide Developer Conference to bring generative AI to the iPhone.\n\n\"We see generative AI as a key opportunity across our products and believe we have advantages that set us apart there,\" Apple CEO Tim Cook said on the company's most recent earnings call in early May.", "source": {"uri": "cnn.com", "dataType": "news", "title": "CNN"}, "authors": [{"uri": "samantha_murphy_kelly@cnn.com", "name": "Samantha Murphy Kelly", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 3, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Viral_phenomenon", "type": "wiki", "score": 3, "label": {"eng": "Viral phenomenon"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Technological_singularity", "type": "wiki", "score": 2, "label": {"eng": "Technological singularity"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/CNN", "type": "org", "score": 2, "label": {"eng": "CNN"}}, {"uri": "http://en.wikipedia.org/wiki/Worldwide_Developers_Conference", "type": "wiki", "score": 1, "label": {"eng": "Worldwide Developers Conference"}}, {"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 1, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Machine_learning", "type": "wiki", "score": 1, "label": {"eng": "Machine learning"}}, {"uri": "http://en.wikipedia.org/wiki/IPhone", "type": "wiki", "score": 1, "label": {"eng": "IPhone"}}, {"uri": "http://en.wikipedia.org/wiki/Tim_Cook", "type": "person", "score": 1, "label": {"eng": "Tim Cook"}}, {"uri": "http://en.wikipedia.org/wiki/Apple_Inc.", "type": "org", "score": 1, "label": {"eng": "Apple Inc."}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Business/Major_Companies", "label": "dmoz/Business/Major Companies", "wgt": 17}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 26}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 19}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 87}], "image": "https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2089836687.jpg?c=16x9&q=w_800,c_fill", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.0980392156862745, "wgt": 190, "relevance": 1}
{"uri": "2024-06-378664257", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:32:46", "dateTime": "2024-06-04T16:32:46Z", "dateTimePub": "2024-06-04T16:17:24Z", "dataType": "news", "sim": 0.7372549176216125, "url": "https://www.wsiltv.com/news/consumer/openai-insiders-open-letter-warns-of-serious-risks-and-calls-for-whistleblower-protections/article_d6557a8e-2914-50f3-933d-4598617aac00.html", "title": "OpenAI insiders' open letter warns of 'serious risks' and calls for whistleblower protections", "body": "(CNN) -- A group of OpenAI insiders are demanding that artificial intelligence companies be far more transparent about AI's \"serious risks\" -- and that they protect employees who voice concerns about the technology they're building.\n\n\"AI companies have strong financial incentives to avoid effective oversight,\" reads the open letter posted Tuesday signed by current and former employees at AI companies including OpenAI, the creator behind the viral ChatGPT tool.\n\nThey also called for AI companies to foster \"a culture of open criticism\" that welcomes, rather than punishes, people who speak up about their concerns, especially as the law struggles to catch up to the quickly advancing technology.\n\nCompanies have acknowledged the \"serious risks\" posed by AI -- from manipulation to a loss of control, known as \"singularity, that could potentially result in human extinction -- but they should be be doing more to educate the public about risks and protective measures, the group wrote.\n\nAs the law currently stands, the AI employees said, they don't believe AI companies will share critical information about the technology voluntarily.\n\nIt's essential, then, for current and former employees to speak up -- and for companies not to enforce \"disparagement\" agreements or otherwise retaliate against those who voice risk-related concerns. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated,\" the group wrote.\n\nTheir letter comes as companies move quickly to implement generative AI tools into their products, while government regulators, companies and consumers grapple with responsible use. Meanwhile many tech experts, researchers and leaders have called for a temporary pause in the AI race, or for the government to step in and create a moratorium.\n\nOpenAI's response\n\nIn response to the letter, OpenAI spokesperson told CNN it is \"proud of our track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk, adding that the company agrees \"rigorous debate is crucial given the significance of this technology.\"\n\nOpenAI noted it has an anonymous integrity hotline and a Safety and Security Committee led by members of its board and safety leaders from the company. The company does not sell personal info, build user profiles, or use that data to target anyone or sell anything.\n\nBut Daniel Ziegler, one of the organizers behind the letter and an early machine-learning engineer who worked at OpenAI between 2018 and 2021, told CNN that it's important to remain skeptical of the company's commitment to transparency.\n\n\"It's really hard to tell from the outside how seriously they're taking their commitments for safety elevations and figuring out societal harms, especially as there is such strong commercial pressures to move very quickly,\" he said. \"It's really important to have the right culture and processes so that employees can speak out in targeted ways when they have concerns.\"\n\nHe hopes more professionals in the AI industry will go public with their concerns as a result of the letter.\n\nMeanwhile, Apple is widely expected to announce a partnership with OpenAI at its annual Worldwide Developer Conference to bring generative AI to the iPhone.\n\n\"We see generative AI as a key opportunity across our products and believe we have advantages that set us apart there,\" Apple CEO Tim Cook said on the company's most recent earnings call in early May.\n\nThe-CNN-Wire\n\n\u2122 & \u00a9 2024 Cable News Network, Inc., a Warner Bros. Discovery Company. All rights reserved.", "source": {"uri": "wsiltv.com", "dataType": "news", "title": "WSIL"}, "authors": [{"uri": "samantha_murphy_kelly@wsiltv.com", "name": "Samantha Murphy Kelly", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 5, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 4, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/CNN", "type": "org", "score": 4, "label": {"eng": "CNN"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 3, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Technological_singularity", "type": "wiki", "score": 3, "label": {"eng": "Technological singularity"}}, {"uri": "http://en.wikipedia.org/wiki/Viral_phenomenon", "type": "wiki", "score": 3, "label": {"eng": "Viral phenomenon"}}, {"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Warner_Bros._Discovery", "type": "wiki", "score": 1, "label": {"eng": "Warner Bros. Discovery"}}, {"uri": "http://en.wikipedia.org/wiki/Worldwide_Developers_Conference", "type": "wiki", "score": 1, "label": {"eng": "Worldwide Developers Conference"}}, {"uri": "http://en.wikipedia.org/wiki/Machine_learning", "type": "wiki", "score": 1, "label": {"eng": "Machine learning"}}, {"uri": "http://en.wikipedia.org/wiki/IPhone", "type": "wiki", "score": 1, "label": {"eng": "IPhone"}}, {"uri": "http://en.wikipedia.org/wiki/Tim_Cook", "type": "person", "score": 1, "label": {"eng": "Tim Cook"}}, {"uri": "http://en.wikipedia.org/wiki/Apple_Inc.", "type": "org", "score": 1, "label": {"eng": "Apple Inc."}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 1, "label": {"eng": "Chief executive officer"}}], "categories": [{"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 19}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 19}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 19}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 87}], "image": null, "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.1137254901960785, "wgt": 188, "relevance": 1}
{"uri": "8161594275", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "18:03:41", "dateTime": "2024-06-04T18:03:41Z", "dateTimePub": "2024-06-04T18:03:09Z", "dataType": "news", "sim": 0.7372549176216125, "url": "https://www.engadget.com/former-openai-google-and-anthropic-workers-are-asking-ai-companies-for-more-whistleblower-protections-175916744.html", "title": "AI workers demand stronger whistleblower protections in open letter", "body": "It was signed by 13 current and former workers with ties to Google, OpenAI and Anthropic\n\nA group of current and former employees from leading AI companies like OpenAI, Google DeepMind and Anthropic have signed an open letter asking for greater transparency and protection from retaliation for those who speak out about the potential concerns of AI. \"So long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public,\" the letter, which was published on Tuesday, says. \"Yet broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues.\"\n\nThe letter comes just a couple of weeks after a Vox investigation revealed OpenAI had attempted to muzzle recently departing employees by forcing them to chose between signing an aggressive non-disparagement agreement, or risk losing their vested equity in the company. After the report, OpenAI CEO Sam Altman called the provision \"genuinely embarrassing\" and claims it has been removed from recent exit documentation, though it's unclear if it remains in force for some employees.\n\nThe 13 signatories include former OpenAI employees Jacob Hinton, William Saunders and Daniel Kokotajlo. Kokotajlo said that he resigned from the company after losing confidence that it would responsibly build artificial general intelligence, a term for AI systems that is as smart or smarter than humans. The letter -- which was endorsed by prominent AI experts Geoffrey Hinton, Yoshua Bengio and Stuart Russell -- expresses grave concerns over the lack of effective government oversight for AI and the financial incentives driving tech giants to invest in the technology. The authors warn that the unchecked pursuit of powerful AI systems could lead to the spread of misinformation, exacerbation of inequality and even the loss of human control over autonomous systems, potentially resulting in human extinction.\n\n\"There is a lot we don't understand about how these systems work and whether they will remain aligned to human interests as they get smarter and possibly surpass human-level intelligence in all areas,\" wrote Kokotajlo on X. \"Meanwhile, there is little to no oversight over this technology. Instead, we rely on the companies building them to self-govern, even as profit motives and excitement about the technology push them to 'move fast and break things.' Silencing researchers and making them afraid of retaliation is dangerous when we are currently some of the only people in a position to warn the public.\"\n\nOpenAI, Google and Anthropic did not immediately respond to request for comment from Engadget. In a statement sent to Bloomberg, an OpenAI spokesperson said the company is proud of its \"track record providing the most capable and safest AI systems\" and it believes in its \"scientific approach to addressing risk.\" It added: \"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"\n\nThe signatories are calling on AI companies to commit to four key principles:\n\nThe letter comes amid growing scrutiny of OpenAI's practices, including the disbandment of its \"superalignment\" safety team and the departure of key figures like co-founder Ilya Sutskever and Jan Leike, who criticized the company's prioritization of \"shiny products\" over safety.", "source": {"uri": "engadget.com", "dataType": "news", "title": "engadget"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 5, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Vox_(website)", "type": "wiki", "score": 3, "label": {"eng": "Vox (website)"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 3, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 3, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 3, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 2, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 2, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 2, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Economic_inequality", "type": "wiki", "score": 2, "label": {"eng": "Economic inequality"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 2, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 1, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Autonomous_robot", "type": "wiki", "score": 1, "label": {"eng": "Autonomous robot"}}, {"uri": "http://en.wikipedia.org/wiki/Engadget", "type": "wiki", "score": 1, "label": {"eng": "Engadget"}}, {"uri": "http://en.wikipedia.org/wiki/Bloomberg_News", "type": "org", "score": 1, "label": {"eng": "Bloomberg News"}}], "categories": [{"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 17}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 19}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 19}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 70}], "image": "https://s.yimg.com/ny/api/res/1.2/ypOPWyh23zIca8caHN1s0g--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA7Y2Y9d2VicA--/https://s.yimg.com/os/creatr-uploaded-images/2024-06/1fa8e330-2298-11ef-8aed-424125c541b8", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.05098039215686279, "wgt": 188, "relevance": 1}
{"uri": "8161611008", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "18:17:29", "dateTime": "2024-06-04T18:17:29Z", "dateTimePub": "2024-06-04T18:16:10Z", "dataType": "news", "sim": 0.7333333492279053, "url": "https://au.pcmag.com/ai/105550/openai-staffers-demand-right-to-warn-the-public-about-ai-dangers", "title": "OpenAI Staffers Demand Right to Warn the Public About AI Dangers", "body": "Today's tech companies only have 'weak obligations' to share information about the risks of AI to world governments, according to a group of current and former OpenAI staffers.\n\nA group of former and current OpenAI staffers wants to ensure that employees can publicly disclose the potential dangers of artificial intelligence.\n\nOn Tuesday, the group of 13 workers published an open letter arguing that AI companies \"have strong financial incentives to avoid effective oversight,\" including whether their technologies might cause societal upheaval.\n\n\"They currently have only weak obligations to share some of this information with governments, and none with civil society,\" the staffers wrote. \"We do not think they can all be relied upon to share it voluntarily.\"\n\nIn response, the workers are calling on AI companies to commit to four principles designed to give their employees a way to notify the public about such dangers if they were ever to arise. Importantly, the first principle would require AI companies to refrain from using contractual agreements to punish employees for speaking out about AI risks.\n\nSeven former OpenAI employees signed the letter, along with four anonymous current staffers. The two other signees include a former Google DeepMind researcher and a current DeepMind staffer. In addition, AI pioneer Geoffrey Hinton endorsed the document.\n\nThe open letter comes a few weeks after OpenAI was criticized for forcing employees to sign NDAs that prevented departing workers from disparaging the company for life. If they did, they'd lose their vested equity. OpenAI later said it ditched the policy after CEO Sam Altman claimed: \"I did not know this was happening and I should have.\"\n\nEx-employees allege that OpenAI's leadership was fully aware of the NDA policy. \"It's concerning that they engaged in these intimidation tactics for so long and only course-corrected under public pressure,\" tweeted Daniel Kokotajlo, a former OpenAI employee who signed the open letter. \"It's also concerning that leaders who signed off on these policies claim they didn't know about them.\"\n\nThe open letter goes on to say it's crucial that employees at today's AI companies can warn the public about potential dangers, especially since \"no effective government oversight of these corporations\" is currently in place. As a result, the group is calling on AI companies to facilitate an anonymous process for workers to raise AI risk concerns with corporate boards, regulators, and independent organizations.\n\nTwo other principles also demand that AI companies \"support a culture of open criticism\" and refrain from punishing employees for publicly sharing \"risk-related confidential information after other processes have failed.\"\n\nOpenAI didn't immediately respond to a request for comment. But last week, the company announced it was forming a new \"Safety and Security Committee\" meant to oversee future AI projects after the previous leaders of OpenAI's long-term safety team resigned. Altman will lead the new committee alongside OpenAI Board Chairman Bret Taylor, co-creator of Google Maps, and Nicole Seligman, general counsel for Sony.\n\nThe same committee will also take input from third-party experts, such as former NSA Cybersecurity Director Rob Joyce and former US Assistant Attorney General for National Security John Carlin.", "source": {"uri": "au.pcmag.com", "dataType": "news", "title": "PCMag Australia"}, "authors": [{"uri": "social_media@au.pcmag.com", "name": "Social Media", "type": "author", "isAgency": false}, {"uri": "i_ve_been@au.pcmag.com", "name": "I've Been", "type": "author", "isAgency": false}, {"uri": "including_consumer_electronics@au.pcmag.com", "name": "Including Consumer Electronics", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 5, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 3, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 2, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 2, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Contract", "type": "wiki", "score": 2, "label": {"eng": "Contract"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 2, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Chairperson", "type": "wiki", "score": 1, "label": {"eng": "Chairperson"}}, {"uri": "http://en.wikipedia.org/wiki/United_States_Assistant_Attorney_General", "type": "wiki", "score": 1, "label": {"eng": "United States Assistant Attorney General"}}, {"uri": "http://en.wikipedia.org/wiki/General_counsel", "type": "wiki", "score": 1, "label": {"eng": "General counsel"}}, {"uri": "http://en.wikipedia.org/wiki/Confidentiality", "type": "wiki", "score": 1, "label": {"eng": "Confidentiality"}}, {"uri": "http://en.wikipedia.org/wiki/National_Security_Agency", "type": "wiki", "score": 1, "label": {"eng": "National Security Agency"}}, {"uri": "http://en.wikipedia.org/wiki/Computer_security", "type": "wiki", "score": 1, "label": {"eng": "Computer security"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 1, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 1, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Sony", "type": "org", "score": 1, "label": {"eng": "Sony"}}, {"uri": "http://en.wikipedia.org/wiki/Twitter", "type": "org", "score": 1, "label": {"eng": "Twitter"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 17}, {"uri": "dmoz/Society/Work/Coworker_Relations", "label": "dmoz/Society/Work/Coworker Relations", "wgt": 17}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 23}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 17}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 21}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 81}], "image": "https://sm.pcmag.com/t/pcmag_au/news/o/openai-sta/openai-staffers-demand-right-to-warn-the-public-about-ai-dan_pwez.1200.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.03529411764705892, "wgt": 187, "relevance": 1}
{"uri": "8161397437", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:39:50", "dateTime": "2024-06-04T15:39:50Z", "dateTimePub": "2024-06-04T15:38:41Z", "dataType": "news", "sim": 0.7333333492279053, "url": "https://economictimes.indiatimes.com/tech/technology/openai-google-deepminds-current-and-former-employees-warn-about-ai-risks/articleshow/110710822.cms", "title": "OpenAI, Google DeepMind's current and former employees warn about AI risks", "body": "The letter further warns of risks from unregulated AI, ranging from the spread of misinformation to the loss of independent AI systems and the deepening of existing inequalities, which could result in \"human extinction.\"A group of current and former employees at artificial intelligence (AI) companies, including Microsoft-backed OpenAI and Alphabet's Google DeepMind on Tuesday raised concerns about risks posed by the emerging technology.\n\nAn open letter by a group of 11 current and former employees of OpenAI and one current and another former employee with Google DeepMind said the financial motives of AI companies hinder effective oversight.\n\n\"We do not believe bespoke structures of corporate governance are sufficient to change this,\" the letter added.\n\nIt further warns of risks from unregulated AI, ranging from the spread of misinformation to the loss of independent AI systems and the deepening of existing inequalities, which could result in \"human extinction.\"\n\nResearchers have found examples of image generators from companies including OpenAI and Microsoft producing photos with voting-related disinformation, despite policies against such content.\n\nAI companies have \"weak obligations\" to share information with the governments about the capabilities and limitations of their systems, the letter said, adding that these firms cannot be relied upon to share that information voluntarily.\n\nThe open letter is the latest to raise safety concerns around generative AI technology, which can quickly and cheaply produce human-like text, imagery and audio.\n\nThe group has urged AI firms to facilitate a process for current and former employees to raise risk-related concerns and not enforce confidentiality agreements that prohibit criticism.\n\nSeparately, the Sam Altman-led firm said on Thursday it disrupted five covert influence operations that sought to use its artificial intelligence models for \"deceptive activity\" across the internet.", "source": {"uri": "economictimes.indiatimes.com", "dataType": "news", "title": "Economic Times"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 4, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 4, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 4, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Emerging_technologies", "type": "wiki", "score": 3, "label": {"eng": "Emerging technologies"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 2, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 1, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 1, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Disinformation", "type": "wiki", "score": 1, "label": {"eng": "Disinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Internet", "type": "wiki", "score": 1, "label": {"eng": "Internet"}}], "categories": [{"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 26}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 17}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 26}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 21}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 70}], "image": "https://img.etimg.com/thumb/msid-110710870,width-1200,height-630,imgsize-654981,overlay-ettech/photo.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.01960784313725494, "wgt": 187, "relevance": 1}
{"uri": "8161498640", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:49:29", "dateTime": "2024-06-04T16:49:29Z", "dateTimePub": "2024-06-04T16:49:03Z", "dataType": "news", "sim": 0.7333333492279053, "url": "https://qz.com/open-letter-from-ai-employees-says-ai-companies-pose-se-1851518657", "title": "Open letter from AI employees says AI companies pose 'serious risks' and need oversight", "body": "Employees of leading AI companies are raising alarm bells about \"serious risks\" posed by the light-speed evolution of artificial intelligence. In an open letter published today, some 13 former and current staffers of OpenAI, GoogleDeepMind, and Anthropic -- 11 of the group were or are affiliated with OpenAI -- warned that AI tech companies lack oversight and transparency.\n\nThey called on companies making cutting edge artificial intelligence technologies to bolster whistleblower protections and create cultures that encourage criticism. Their letter was endorsed by the so-called \"godfathers\" of AI -- Yoshua Bengio and Geoffrey Hinton -- as well as computer scientist Stuart Russell, all of whom contributed instrumental research that led to the creation of modern AI and then became some of its biggest critics.\n\nWe are current and former employees at frontier AI companies, and we believe in the potential of AI technology to deliver unprecedented benefits to humanity.\n\nWe also understand the serious risks posed by these technologies. These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction. AI companies themselves have acknowledged these risks, as have governments across the world and other AI experts.\n\nWe are hopeful that these risks can be adequately mitigated with sufficient guidance from the scientific community, policymakers, and the public. However, AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this.\n\nAI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm. However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\n\nCurrently, there is no federal regulation governing AI.\n\nSo long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public. Yet broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues.\n\nOpenAI employees are bound by strict confidentiality agreements. and those who choose not to sign those agreements lose their equity in OpenAI. Daniel Kokotajlo is one of the seven authors of the letter who signed their names rather than opting for anonymity. Kokotajlo is a researcher who left OpenAI last month \"due to losing confidence that it would behave responsibly around the time of AGI [an acronym for a more advanced form of artificial intelligence called \"artificial general intelligence].\"\n\nKokotajlo revealed in a post on his website last month that he opted out of the confidentiality agreement, losing his equity, so that he could openly criticize the company.", "source": {"uri": "qz.com", "dataType": "news", "title": "Quartz"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 3, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 3, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 3, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 3, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Computer_scientist", "type": "wiki", "score": 3, "label": {"eng": "Computer scientist"}}, {"uri": "http://en.wikipedia.org/wiki/Transparency_(behavior)", "type": "wiki", "score": 3, "label": {"eng": "Transparency (behavior)"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 3, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Evolution", "type": "wiki", "score": 3, "label": {"eng": "Evolution"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 2, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Corporate_governance", "type": "wiki", "score": 2, "label": {"eng": "Corporate governance"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 1, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 1, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Anonymity", "type": "wiki", "score": 1, "label": {"eng": "Anonymity"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 1, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 1, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 1, "label": {"eng": "Corporation"}}], "categories": [{"uri": "dmoz/Computers/Artificial_Intelligence/Associations", "label": "dmoz/Computers/Artificial Intelligence/Associations", "wgt": 22}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 31}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 30}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 25}], "image": "https://i.kinja-img.com/image/upload/c_fill,h_675,pg_1,q_80,w_1200/61092ada1238e2a19744647055b367c0.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.03529411764705881, "wgt": 187, "relevance": 1}
{"uri": "8161329838", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "14:55:40", "dateTime": "2024-06-04T14:55:40Z", "dateTimePub": "2024-06-04T14:54:14Z", "dataType": "news", "sim": 0.7254902124404907, "url": "https://winbuzzer.com/2024/06/04/openai-faces-allegations-of-reckless-ai-development-xcxwbn/", "title": "Whistleblowers Criticize OpenAI's Approach to AI Safety, DeepMind Employees Lend Support - WinBuzzer", "body": "Former OpenAI researchers and current DeepMind employees have written the Right to Warn open letter to push for more AI safety.\n\nA group of current and former employees of OpenAI and Google DeepMind have leveled serious accusations against the company's internal practices and approach to AI technology. Comprising nine former staff members from OpenAI and two DeepMind employees, the whistleblowers say in an open letter that OpenAI's aggressive push for growth and profits is overshadowing critical concerns about safety and transparency, particularly as the company aims to develop artificial general intelligence (AGI).\n\nOne of the notable insiders, Daniel Kokotajlo, formerly a researcher in OpenAI's governance sector, alleges a reckless agenda to quickly achieve AGI, an advanced form of AI capable of human-like cognitive tasks. According to these accounts, OpenAI uses stringent nondisparagement agreements to stifle internal dissent and appears more focused on rapid advancements than on ethical considerations.\n\nThe group has publicly appealed for AI firms, including OpenAI, to enhance transparency and strengthen whistleblower protections. This open letter urges these companies to ensure their AI advancements are both ethical and responsible. The employees point out that the current atmosphere at OpenAI could foster the development of hazardous AI systems if left unchecked.\n\nOpenAI began as a nonprofit research entity and came into the spotlight with the 2022 release of ChatGPT. Since then, it has shifted towards increasing the sophistication of its AI technologies. The whistleblowers claim this change has fostered a culture focused on rapid development and profitability at the expense of ethical standards and safety protocols.\n\nThose speaking out highlight the possible dangers associated with AI innovations, such as reinforcing societal inequities, spreading misinformation, and losing control over autonomous AI, which could have catastrophic consequences. They maintain that financial motives drive some AI companies to resist rigorous oversight, while existing corporate governance mechanisms fall short in mitigating these threats.\n\nThe open letter has garnered support from leading AI authorities such as Yoshua Bengio, Geoffrey Hinton, and Stuart Russell. Among the signatories are key individuals like Jacob Hilton, Daniel Kokotajlo, Ramana Kumar, Neel Nanda, William Saunders, Carroll Wainwright, and Daniel Ziegler. The document references various acknowledgments of AI risks from organizations such as OpenAI, Anthropic, Google, and numerous government bodies, along with international declarations from AI research groups.\n\nThe group insists that AI companies commit to several key principles: refraining from enforcing nondisparagement clauses, enabling anonymous risk reporting, and promoting a culture that encourages open critique without fear of retaliation. They argue that these companies hold extensive non-public information about their systems' capabilities and risks, information that is crucial for civil oversight.", "source": {"uri": "winbuzzer.com", "dataType": "news", "title": "WinBuzzer"}, "authors": [{"uri": "luke_jones@winbuzzer.com", "name": "Luke Jones", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 5, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Ethics", "type": "wiki", "score": 4, "label": {"eng": "Ethics"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 3, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 2, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Cognition", "type": "wiki", "score": 2, "label": {"eng": "Cognition"}}, {"uri": "http://en.wikipedia.org/wiki/Nonprofit_organization", "type": "wiki", "score": 2, "label": {"eng": "Nonprofit organization"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 1, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 1, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 1, "label": {"eng": "Google"}}], "categories": [{"uri": "dmoz/Computers/Artificial_Intelligence", "label": "dmoz/Computers/Artificial Intelligence", "wgt": 16}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 26}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 29}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 23}], "image": "https://winbuzzer.com/wp-content/uploads/2023/02/openai-logo.png", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.3019607843137255, "wgt": 185, "relevance": 1}
{"uri": "8161566292", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:42:27", "dateTime": "2024-06-04T17:42:27Z", "dateTimePub": "2024-06-04T17:41:43Z", "dataType": "news", "sim": 0.7254902124404907, "url": "https://time.com/6985504/openai-google-deepmind-employees-letter/", "title": "Employees Say OpenAI and Google DeepMind Are Hiding Dangers from the Public", "body": "A group of current and former employees at leading AI companies OpenAI and Google DeepMind published a letter on Tuesday warning against the dangers of advanced AI as they allege companies are prioritizing financial gains while avoiding oversight.\n\nThirteen employees, eleven of which are current or former employees of OpenAI, the company behind ChatGPT, signed the letter entitled: \"A Right to Warn about Advanced Artificial Intelligence.\" The two other signatories are current and former employees of Google DeepMind. Six individuals are anonymous.\n\nThe coalition cautions that AI systems are powerful enough to pose serious harms without proper regulation. \"These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction,\" the letter says.\n\nRead More: Exclusive: U.S. Must Move 'Decisively' to Avert 'Extinction-Level' Threat From AI, Government-Commissioned Report Says\n\n\"We're proud of our track record providing the most capable and safest A.I. systems and believe in our scientific approach to addressing risk,\" OpenAI spokeswoman Lindsey Held told the New York Times. \"We agree that rigorous debate is crucial given the significance of this technology, and we'll continue to engage with governments, civil society and other communities around the world,\"\n\nGoogle DeepMind has not commented publicly on the letter and did not respond to TIME's request for comment.\n\nLeaders of all three leading AI companies -- OpenAI, Google DeepMind and Anthropic -- have talked about the risks in the past. \"If we build an AI system that's significantly more competent than human experts but it pursues goals that conflict with our best interests, the consequences could be dire... rapid AI progress would be very disruptive, changing employment, macroeconomics, and power structures ... [we have already encountered] toxicity, bias, unreliability, dishonesty,\" AI safety and research company Anthropic said in a March 2023 statement, which is linked to in the letter. (One of the letter signatories who currently works at Google DeepMind used to work at Anthropic.)\n\nRead More: Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\n\nThe group behind the letter alleges that AI companies have information about the risks of the AI technology they are working on, but because they aren't required to disclose much with governments, the real capabilities of their systems remain a secret. That means current and former employees are the only ones who can hold the companies accountable to the public, they say, and yet many have found their hands tied by confidentiality agreements that prevent workers from voicing their concerns publicly. \"Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated,\" the group wrote.\n\n\"Employees are an important line of safety defense, and if they can't speak freely without retribution, that channel's going to be shut down,\" the group's pro bono lawyer Lawrence Lessig told the New York Times.\n\nThe letter writers have made four demands of advanced AI companies: stop forcing employees into agreements that prevent them from criticizing their employer for \"risk-related concerns,\" create an anonymous process for employees to raise their concerns to board members and other relevant regulators or organizations, support a \"culture of open criticism,\" and not retaliate against former and current employees who share \"risk-related confidential information after other processes have failed.\"\n\nGovernments around the world have moved to regulate AI, though progress lags behind the speed at which AI is progressing. Earlier this year, the E.U. passed the world's first comprehensive AI legislation. Efforts at international cooperation have been pursued through AI Safety Summits in the U.K. and South Korea, and at the U.N. In October 2023. President Joe Biden signed an AI executive order that, among other things, requires AI companies to disclose their development and safety testing plans to the Department of Commerce. However, these disclosures are not required to be made public -- potentially preventing the broader oversight that the letter's signatories want.\n\n-With additional reporting by Will Henshall/Washington", "source": {"uri": "time.com", "dataType": "news", "title": "TIME"}, "authors": [{"uri": "solcyre_burga@time.com", "name": "Solcyr\u00e9 Burga", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 5, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 5, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 3, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 3, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 2, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 2, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Macroeconomics", "type": "wiki", "score": 2, "label": {"eng": "Macroeconomics"}}, {"uri": "http://en.wikipedia.org/wiki/Bias", "type": "wiki", "score": 2, "label": {"eng": "Bias"}}, {"uri": "http://en.wikipedia.org/wiki/Executive_order_(United_States)", "type": "wiki", "score": 1, "label": {"eng": "Executive order (United States)"}}, {"uri": "http://en.wikipedia.org/wiki/Lawrence_Lessig", "type": "person", "score": 1, "label": {"eng": "Lawrence Lessig"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 1, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Pro_bono", "type": "wiki", "score": 1, "label": {"eng": "Pro bono"}}, {"uri": "http://en.wikipedia.org/wiki/Joe_Biden", "type": "person", "score": 1, "label": {"eng": "Joe Biden"}}, {"uri": "http://en.wikipedia.org/wiki/United_States_Department_of_Commerce", "type": "wiki", "score": 1, "label": {"eng": "United States Department of Commerce"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 1, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/United_Nations", "type": "org", "score": 1, "label": {"eng": "United Nations"}}, {"uri": "http://en.wikipedia.org/wiki/European_Union", "type": "loc", "score": 1, "label": {"eng": "European Union"}, "location": null}, {"uri": "http://en.wikipedia.org/wiki/Washington,_D.C.", "type": "loc", "score": 1, "label": {"eng": "Washington, D.C."}, "location": {"type": "place", "label": {"eng": "Washington, D.C."}, "country": {"type": "country", "label": {"eng": "United States"}}}}, {"uri": "http://en.wikipedia.org/wiki/South_Korea", "type": "loc", "score": 1, "label": {"eng": "South Korea"}, "location": {"type": "country", "label": {"eng": "South Korea"}}}, {"uri": "http://en.wikipedia.org/wiki/United_Kingdom", "type": "loc", "score": 1, "label": {"eng": "United Kingdom"}, "location": {"type": "country", "label": {"eng": "United Kingdom"}}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 18}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 22}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 27}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 19}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 72}], "image": "https://api.time.com/wp-content/uploads/2024/06/GettyImages-2155035826.jpg?quality=85&w=1024&h=628&crop=1", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.01960784313725483, "wgt": 185, "relevance": 1}
{"uri": "8161361402", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:15:16", "dateTime": "2024-06-04T15:15:16Z", "dateTimePub": "2024-06-04T15:14:34Z", "dataType": "news", "sim": 0.7098039388656616, "url": "https://futurism.com/openai-insiders-silenced", "title": "OpenAI Insiders Say They're Being Silenced About Danger", "body": "A coalition of current and former employees of OpenAI -- as well as a handful of colleagues from Anthropic and Google DeepMind -- are pushing for a \"Right to Warn\" AI leadership and the public about AI safety concerns, citing fears over accountability, company overreach, and the silencing of AI workers.\n\nThe group is seeking robust whistleblower protections, safe anonymous reporting pathways, and the abolition of the restrictive non-disclosure and non-disparagement agreements that quiet current and former AI staffers. The AI workers are also asking that AI companies work to \"support a culture of open criticism,\" as they write in an open letter, so long as trade secrets are protected.\n\nAccording to the letter and a press release, the Right to Warn demands have been cosigned by AI \"godfathers\" Yoshua Bengio and Geoffrey Hinton, as well as fellow renowned AI scientist Stuart Russell.\n\nPer the cohort's website, they firmly believe that AI will \"deliver unprecedented benefits to humanity.\" But they also urge that tech certainly doesn't come without its risks, including the concentration of power within the industry and the silencing of concerned staffers.\n\n\"OpenAI CEO Sam Altman has said, 'you should not trust one company and certainly not one person [to govern AI],' and we agree,\" said Willliam Saunders, a former OpenAI employee and coalition member, in a statement. \"When dealing with potentially dangerous new technologies, there should be ways to share information about risks with independent experts, governments, and the public.\"\n\n\"Today, the people with the most knowledge about how frontier AI systems work and the risks related to their deployment,\" he continued, \"are not fully free to speak because of possible retaliation and overly broad confidentiality agreements.\"\n\nThe announcement of the Right to Warn initiative comes on the heels of a damning Vox report revealing that ChatGPT creator OpenAI was threatening to claw back employees' vested equity -- which many Silicon Valley workers will accept in lieu of a higher salary -- if they didn't sign heavily restrictive NDAs.\n\nIn response to that initial report, OpenAI CEO Sam Altman claimed he had no knowledge of the vested-equity-for-silence clause, saying he was \"genuinely embarrassed\" it existed. A follow-up Vox piece, however, showed that Altman and other OpenAI executives signed paperwork implying their direct knowledge of the deeply unconventional provision.\n\n\"In order for OpenAI and other AI companies to be held accountable to their own commitments on safety, security, governance and ethics,\" wrote Jacob Hilton, a former OpenAI employee and currently a researcher at the Alignment Research Center, in a Twitter thread, \"the public must have confidence that employees will not be retaliated against for speaking out.\"\n\nHilton added that, as it stands, the \"main way for AI companies to provide assurances to the public is through voluntary public commitments.\" But as the AI researcher noted, this is inherently pretty flimsy, as there's \"no good way for the public to tell if the company is actually sticking to these commitments, and no incentive for the company to be transparent.\"\n\nOn that note, it's also worth mentioning that OpenAI recently disbanded its \"Superalignment\" safety team entirely, and saw several high-profile researchers exit. Which, of course, doesn't exactly sow confidence in the firm's prioritization of safety and ethics efforts. Elsewhere, Google's demonstrably unsafe search AI has had a rough few weeks, too.\n\nIn response to the Right to Warn letter, a spokesperson for OpenAI told The New York Times that the AI company is \"proud of our track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk.\"\n\n\"We agree that rigorous debate is crucial given the significance of this technology,\" the spokesperson added, \"and we'll continue to engage with governments, civil society and other communities around the world.\" They also noted that OpenAI has an anonymous integrity \"hotline.\"\n\nA Google spokesperson, according to the NYT, declined to respond. Futurism has also reached out to Anthropic.\n\nSafety and ethics are important considerations for any burgeoning technology. Given that the leaders of AI companies often talk about the fact that their tech could possibly destroy the entire world, sway elections, or otherwise wreak some short-to-long-term havoc on humanity, that feels especially true in the lucrative and concentrated AI bubble.\n\nBut safety may also mean going slower, something that's very much disincentivized in Silicon Valley's AI race -- and that reality, this letter suggests, is reflected behind the walls of AI companies, where real and productive dialogue about AI safety and the freedom to speak out about possible AI risks and harms might not always be a given.\n\nAnyway... you think Meta's mad they didn't get a shoutout here?", "source": {"uri": "futurism.com", "dataType": "news", "title": "Futurism"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 3, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 3, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Vox_(website)", "type": "wiki", "score": 3, "label": {"eng": "Vox (website)"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 3, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 3, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 3, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 3, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 3, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 3, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 2, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 2, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Ethics", "type": "wiki", "score": 2, "label": {"eng": "Ethics"}}, {"uri": "http://en.wikipedia.org/wiki/Silicon_Valley", "type": "loc", "score": 2, "label": {"eng": "Silicon Valley"}, "location": null}, {"uri": "http://en.wikipedia.org/wiki/Twitter", "type": "org", "score": 2, "label": {"eng": "Twitter"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 1, "label": {"eng": "The New York Times"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 1, "label": {"eng": "Google"}}], "categories": [{"uri": "dmoz/Health/Occupational_Health_and_Safety", "label": "dmoz/Health/Occupational Health and Safety", "wgt": 15}, {"uri": "dmoz/Business/Human_Resources/Training_and_Safety", "label": "dmoz/Business/Human Resources/Training and Safety", "wgt": 16}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 22}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 28}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 21}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 51}], "image": "https://wp-assets.futurism.com/2024/06/openai-insiders-silenced.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.2392156862745098, "wgt": 181, "relevance": 1}
{"uri": "8161346310", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:07:15", "dateTime": "2024-06-04T15:07:15Z", "dateTimePub": "2024-06-04T15:05:11Z", "dataType": "news", "sim": 0.6980392336845398, "url": "https://www.businessinsider.nl/openai-employees-are-demanding-change-here-are-the-4-things-they-want/", "title": "OpenAI employees are demanding change. Here are the 4 things they want.", "body": "Current and former employees at top AI companies are speaking out about the risks of AI. At least 9 OpenAI insiders signed the letter calling for more protection for whistleblowers. Their open letter, which has 4 demands, was endorsed by the \"Godfathers of AI.\"\n\nA group of nine current and former OpenAI employees signed a letter calling out tech firms over major concerns about the risks of artificial intelligence.\n\nIn their letter, the tech workers called for more transparency in AI companies and better protections for whistleblowers who wish to raise concerns about the power of AI.\n\n\"We are current and former employees at frontier AI companies, and we believe in the potential of AI technology to deliver unprecedented benefits to humanity,\" the letter said.\n\n\"We also understand the serious risks posed by these technologies. These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction,\" it continued. \"AI companies themselves have acknowledged these risks, as have governments across the world, and other AI experts.\"\n\nA total of 13 people signed the letter, and they all come from some of the top players in AI -- including OpenAI, Anthropic, and DeepMind. It was also endorsed by two men known as the \"Godfathers of AI,\" Yoshua Bengio and Geoffrey Hinton.\n\n\"I decided to leave OpenAI because I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence,\" former OpenAI employee Daniel Kokotajlo said in a statement.\n\n\"They and others have bought into the 'move fast and break things' approach and that is the opposite of what is needed for technology this powerful and this poorly understood,\" he added.\n\nThe AI employees outlined a list of four demands that they said would help mitigate the existing issues of inequality and misinformation in the AI space.\n\nHere's a look at the four principles the 13 employees said they want OpenAI and other AI companies to adopt, according to the letter.\n\nThat the company will not enter into or enforce any agreement that prohibits \"disparagement\" or criticism of the company for risk-related concerns, nor retaliate for risk-related criticism by hindering any vested economic benefit; That the company will facilitate a verifiably anonymous process for current and former employees to raise risk-related concerns to the company's board, to regulators, and to an appropriate independent organization with relevant expertise; That the company will support a culture of open criticism and allow its current and former employees to raise risk-related concerns about its technologies to the public, to the company's board, to regulators, or to an appropriate independent organization with relevant expertise, so long as trade secrets and other intellectual property interests are appropriately protected; That the company will not retaliate against current and former employees who publicly share risk-related confidential information after other processes have failed. We accept that any effort to report risk-related concerns should avoid releasing confidential information unnecessarily. Therefore, once an adequate process for anonymously raising concerns to the company's board, to regulators, and to an appropriate independent organization with relevant expertise exists, we accept that concerns should be raised through such a process initially. However, as long as such a process does not exist, current and former employees should retain their freedom to report their concerns to the public.\n\nBusiness Insider has reached out to OpenAI, Antrhopic, and Google Deepmind for comment on the letter.\n\nOpenAI spokesperson Lindsey Held told The New York Times that the company is \"proud of our track record providing the most capable and safest A.I. systems and believe in our scientific approach to addressing risk.\"\n\n\"We agree that rigorous debate is crucial given the significance of this technology, and we'll continue to engage with governments, civil society and other communities around the world,\" the statement continued.\n\nRead the original article on Business Insider", "source": {"uri": "businessinsider.nl", "dataType": "news", "title": "Business Insider Nederland"}, "authors": [{"uri": "jordan_hart@businessinsider.nl", "name": "Jordan Hart", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 5, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 2, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 2, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Economic_inequality", "type": "wiki", "score": 2, "label": {"eng": "Economic inequality"}}, {"uri": "http://en.wikipedia.org/wiki/Business_Insider", "type": "wiki", "score": 1, "label": {"eng": "Business Insider"}}, {"uri": "http://en.wikipedia.org/wiki/Intellectual_property", "type": "wiki", "score": 1, "label": {"eng": "Intellectual property"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 1, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/The_New_York_Times", "type": "wiki", "score": 1, "label": {"eng": "The New York Times"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 19}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 19}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 23}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 23}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 47}], "image": "https://i.insider.com/664f6822a961b37edf394160?format=jpeg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.02745098039215677, "wgt": 178, "relevance": 1}
{"uri": "8161526816", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:11:28", "dateTime": "2024-06-04T17:11:28Z", "dateTimePub": "2024-06-04T17:10:49Z", "dataType": "news", "sim": 0.6980392336845398, "url": "https://www.investing.com/news/stock-market-news/openai-google-deepminds-current-and-former-employees-warn-about-ai-risks-3470228", "title": "OpenAI, Google DeepMind's current and former employees warn about AI risks By Reuters", "body": "(Reuters) - A group of current and former employees at artificial intelligence (AI) companies, including Microsoft-backed OpenAI and Alphabet (NASDAQ:GOOGL)'s Google DeepMind on Tuesday raised concerns about risks posed by the emerging technology.\n\nAn open letter by a group of 11 current and former employees of OpenAI and one current and another former employee with Google DeepMind said the financial motives of AI companies hinder effective oversight.\n\n\"We do not believe bespoke structures of corporate governance are sufficient to change this,\" the letter added.\n\nIt further warns of risks from unregulated AI, ranging from the spread of misinformation to the loss of independent AI systems and the deepening of existing inequalities, which could result in \"human extinction.\"\n\nResearchers have found examples of image generators from companies including OpenAI and Microsoft (NASDAQ:MSFT) producing photos with voting-related disinformation, despite policies against such content.\n\nAI companies have \"weak obligations\" to share information with the governments about the capabilities and limitations of their systems, the letter said, adding that these firms cannot be relied upon to share that information voluntarily.\n\nThe open letter is the latest to raise safety concerns around generative AI technology, which can quickly and cheaply produce human-like text, imagery and audio.\n\nThe group has urged AI firms to facilitate a process for current and former employees to raise risk-related concerns and not enforce confidentiality agreements that prohibit criticism.\n\nSeparately, the Sam Altman-led firm said on Thursday it disrupted five covert influence operations that sought to use its artificial intelligence models for \"deceptive activity\" across the internet.", "source": {"uri": "investing.com", "dataType": "news", "title": "Investing.com"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 4, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Alphabet_Inc.", "type": "org", "score": 3, "label": {"eng": "Alphabet Inc."}}, {"uri": "http://en.wikipedia.org/wiki/Emerging_technologies", "type": "wiki", "score": 3, "label": {"eng": "Emerging technologies"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 2, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Disinformation", "type": "wiki", "score": 2, "label": {"eng": "Disinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 2, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 1, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 1, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Internet", "type": "wiki", "score": 1, "label": {"eng": "Internet"}}, {"uri": "http://en.wikipedia.org/wiki/Reuters", "type": "wiki", "score": 1, "label": {"eng": "Reuters"}}], "categories": [{"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 21}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 83}], "image": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEK530O1_M.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.06666666666666665, "wgt": 178, "relevance": 1}
{"uri": "8161596479", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "18:05:21", "dateTime": "2024-06-04T18:05:21Z", "dateTimePub": "2024-06-04T18:04:42Z", "dataType": "news", "sim": 0.686274528503418, "url": "https://www.rocketnews.com/2024/06/former-openai-employees-lead-push-to-protect-whistleblowers-flagging-artificial-intelligence-risks/", "title": "Former OpenAI employees lead push to protect whistleblowers flagging artificial intelligence risks - RocketNews", "body": "A group of OpenAI's current and former workers is calling on the ChatGPT-maker and other artificial intelligence companies to protect whistleblowing employees who flag safety risks about AI technology.An open letter published Tuesday asks tech companies to establish stronger whistleblower protections so researchers can raise concerns about the development of high-performing AI systems internally and with the public without fear of retaliation.Former OpenAI employee Daniel Kokotajlo, who left the company earlier this year, said in a written statement that tech companies are \"disregarding the risks and impact of AI\" as they race to develop better-than-human AI systems known as artificial general intelligence.\"I decided to leave OpenAI because I lost hope that they would act responsibly, particularly as they pursue artificial general intelligence,\" he wrote. \"They and others have bought into the 'move fast and break things' approach and that is the opposite of what is needed for technology this powerful and this poorly understood.\"OpenAI said in a statement responding to the letter that it already has measures for employees to express concerns, including an anonymous integrity hotline.\"We're proud of our track record providing the most capable and safest AI systems and believe in our scientific approach to addressing risk,\" said the company's statement. \"We agree that rigorous debate is crucial given the significance of this technology and we'll continue to engage with governments, civil society and other communities around the world.\"The letter has 13 signatories, most of whom are former employees o ...", "source": {"uri": "rocketnews.com", "dataType": "news", "title": "RocketNews | Top News Stories From Around the Globe"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 3, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Hotline", "type": "wiki", "score": 1, "label": {"eng": "Hotline"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 1, "label": {"eng": "Civil society"}}], "categories": [{"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 31}, {"uri": "dmoz/Computers/Artificial_Intelligence/Associations", "label": "dmoz/Computers/Artificial Intelligence/Associations", "wgt": 30}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 35}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 28}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 27}], "image": null, "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.5529411764705883, "wgt": 175, "relevance": 1}
{"uri": "8161606639", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "18:13:19", "dateTime": "2024-06-04T18:13:19Z", "dateTimePub": "2024-06-04T18:12:46Z", "dataType": "news", "sim": 0.658823549747467, "url": "https://www.thedrum.com/news/2024/06/04/ai-researchers-with-ties-openai-and-google-speak-up-whistleblower-protections", "title": "AI researchers with ties to OpenAI and Google speak up for whistleblower protections", "body": "The authors of the letter write that AI has the power \"to deliver unprecedented benefits to humanity,\" but only if it's developed with adequate guardrails - along with reliable channels of communication through which individuals with insider knowledge can voice their concerns. If built with too cavalier an attitude around safety and security, the authors write, advanced AI systems could eventually spiral out of control, \"potentially resulting in human extinction.\"\n\nThe companies building AI are motivated by the rewards of what is already shaping up to be an immensely lucrative industry. On the other hand, today's open letter argue, those same companies don't have much incentive to be transparent about the potential safety risks of the AI-powered products they're bringing to market.\n\n\"AI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm,\" the letter states. \"However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\"\n\nThe letter points out that traditional whistleblower protections, which typically apply to cases involving illegal activity, would not cover the AI industry, which is currently unregulated.\n\nThe letter was signed by 13 people with current or former ties to OpenAI and Google DeepMind, most of whom remained anonymous. It was also endorsed by AI luminaries Geoffrey Hinton (who left his post at Google last year to bring attention to the issue of AI safety), Yoshua Bengio and Stuart Russell.\n\nOpenAI has struggled to remain in the good graces of public opinion since it was launched into the cultural spotlight following the release of ChatGPT in 2022. While it's received billions of dollars in investments from Microsoft and come to be viewed as one of the leaders in the race to build artificial general intelligence, or AGI, it's also attracted the ire of artists and news publishers, who claim that their intellectual property has been stolen by the company to train large language models.\n\nJan Leike and Ilya Sutskever, both of whom had previously led OpenAI's superalignment team - which focused on optimizing the safety of AI models - both left the company last month. Leike, who has since been hired by Anthropic, accused his former employer in an X post of sidelining safety as it rushed headlong to launch new products.\n\nThen, on May 18, Vox published a report drawing attention to non-disclosure and non-disparagement clauses found in employee off-boarding documents at OpenAI. Violating or refusing to sign those clauses could reportedly result in employees forfeiting their vested equity in the company, which for some is valued in the millions of dollars.\n\nOpenAI's PR challenges became even more severe two days later when actress Scarlett Johanssen issued a public statement effectively claiming that her voice had been used without consent by OpenAI to train a new model called GPT-4o. (OpenAI has denied that the model's voice was based on Johansson.)\n\nIncidents like these have led to a perception among critics of some leading AI companies - and OpenAI in particular - as reckless juggernauts which, operating in a legal vacuum, have proceeded to steamroll over individuals and institutions in an effort to capture market share. And it's exactly this scenario, the authors of the new open letter argue, which necessitates mechanisms to protect whistleblowers.\n\n\"So long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public,\" the letter reads.\n\nOpenAI announced last week that it had formed a new safety and security committee comprised of company board members. The company has not responded to The Drum's request for comment.", "source": {"uri": "thedrum.com", "dataType": "news", "title": "The Drum"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 3, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 2, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Large_language_model", "type": "wiki", "score": 2, "label": {"eng": "Large language model"}}, {"uri": "http://en.wikipedia.org/wiki/ChatGPT", "type": "wiki", "score": 2, "label": {"eng": "ChatGPT"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 2, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 2, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 2, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Intellectual_property", "type": "wiki", "score": 2, "label": {"eng": "Intellectual property"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 2, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 2, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 1, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 1, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Vox_(website)", "type": "wiki", "score": 1, "label": {"eng": "Vox (website)"}}, {"uri": "http://en.wikipedia.org/wiki/Perception", "type": "wiki", "score": 1, "label": {"eng": "Perception"}}, {"uri": "http://en.wikipedia.org/wiki/Equity_(finance)", "type": "wiki", "score": 1, "label": {"eng": "Equity (finance)"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 1, "label": {"eng": "Corporation"}}], "categories": [{"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 24}, {"uri": "dmoz/Business/Human_Resources/Training_and_Safety", "label": "dmoz/Business/Human Resources/Training and Safety", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 21}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 22}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 18}], "image": "https://thedrum-media.imgix.net//thedrum-prod/s3/news/352828/untitled_design_-_2024-06-04t115304.374.png?w=1280&ar=default&fit=crop&crop=faces&auto=format", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": [{"amb": false, "imp": true, "date": "2024-05-18", "textStart": 2564, "textEnd": 2570}], "sentiment": 0.1686274509803922, "wgt": 168, "relevance": 1}
{"uri": "2024-06-378707554", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:17:21", "dateTime": "2024-06-04T17:17:21Z", "dateTimePub": "2024-06-04T13:08:00Z", "dataType": "news", "sim": 0.6509804129600525, "url": "https://www.proactiveinvestors.com/companies/news/1049118/openai-google-deepmind-employees-call-for-oversight-whisteblower-protection-to-curb-ai-risks-1049118.html", "title": "OpenAI, Google Deepmind employees call for oversight, whisteblower protection to curb AI risks", "body": "Current and former staff at AI firms OpenAI, Anthropic and Google Deepmind have published an open letter describing their concerns about the rapid advancement of AI with a lack of oversight and the absence of whistleblower protection.\n\nIn the letter, the AI professionals cited numerous risks from AI technologies including the further entrenchment of existing inequalities, manipulation and misinformation, and the loss of control of autonomous AI systems potentially resulting in human extinction.\n\nThey wrote that they believe these risks can be mitigated but AI companies have \"strong financial incentives to avoid effective oversight and we do not believe bespoke structures of corporate governance are sufficient to change this.\"\n\nThey called upon AI companies to commit to several principles, including facilitating an anonymous process for current and former employees to raise concerns to the company's board, regulators and appropriate independent organizations and for these companies to support a culture of open criticism.\n\nThey also urged companies not to retaliate against current and former employees who publicly share risk-related confidential information after other processes have failed.\n\nThey noted that ordinary whistleblower protections are \"insufficient\" because they are focused on illegal activity, whereas the risks they are concerned about are not yet regulated.\n\n\"So long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public,\" they wrote.\n\n\"Some of us reasonably fear various forms of retaliation, given the history of such cases across the industry. We are not the first to encounter or speak about these issues.\"", "source": {"uri": "proactiveinvestors.com", "dataType": "news", "title": "Proactiveinvestors NA"}, "authors": [{"uri": "emily_jarvie@proactiveinvestors.com", "name": "Emily Jarvie", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 5, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 3, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 3, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 2, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Corporate_governance", "type": "wiki", "score": 2, "label": {"eng": "Corporate governance"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 1, "label": {"eng": "Corporation"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 15}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 23}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 16}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 21}], "image": "https://cdn.proactiveinvestors.com/eyJidWNrZXQiOiJwYS1jZG4iLCJrZXkiOiJ1cGxvYWRcL05ld3NcL0ltYWdlXC8yMDI0XzA2XC8yMDI0LTA0LTAzLTEyLTIzLTU4LTU2YTI0MzBiYzYyNTg2ODY3MDJiNzA1ZjcwYTZjYjViLmpwZyIsImVkaXRzIjp7InJlc2l6ZSI6eyJ3aWR0aCI6MTIwMCwiaGVpZ2h0Ijo2MzAsImZpdCI6ImNvdmVyIn19fQ==", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.04313725490196074, "wgt": 166, "relevance": 1}
{"uri": "8161505686", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:55:12", "dateTime": "2024-06-04T16:55:12Z", "dateTimePub": "2024-06-04T16:54:52Z", "dataType": "news", "sim": 0.6431372761726379, "url": "https://www.theverge.com/2024/6/4/24171283/openai-safety-open-letter-whistleblower-agi", "title": "Former OpenAI employees say whistleblower protection on AI safety is not enough", "body": "Several former OpenAI employees warned in an open letter that advanced AI companies like OpenAI stifle criticism and oversight, especially as concerns over AI safety have increased in the past few months.\n\nThe open letter, signed by 13 former OpenAI employees (six of whom chose to remain anonymous) and endorsed by \"Godfather of AI\" Geoffrey Hinton, formerly of Google, says that in the absence of any effective government oversight, AI companies should commit to open criticism principles. These principles include avoiding the creation and enforcement of non-disparagement clauses, facilitating a \"verifiably\" anonymous process to report issues, allowing current and former employees to raise concerns to the public, and not retaliating against whistleblowers.", "source": {"uri": "theverge.com", "dataType": "news", "title": "The Verge"}, "authors": [{"uri": "emilia_david@theverge.com", "name": "Emilia David", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 4, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 3, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 2, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 2, "label": {"eng": "Google"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 14}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 29}, {"uri": "dmoz/Society/Work/Coworker_Relations", "label": "dmoz/Society/Work/Coworker Relations", "wgt": 15}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 19}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 54}], "image": "https://cdn.vox-cdn.com/thumbor/-mE7YL804Hm6rTh4Jjz4uCxwsf0=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/25263502/STK_414_AI_B.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.3254901960784313, "wgt": 164, "relevance": 1}
{"uri": "2024-06-378613161", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:43:41", "dateTime": "2024-06-04T15:43:41Z", "dateTimePub": "2024-06-04T11:17:00Z", "dataType": "news", "sim": 0.6392157077789307, "url": "https://www.reuters.com/technology/openai-google-deepminds-current-former-employees-warn-about-ai-risks-2024-06-04/", "title": "OpenAI, Google DeepMind's current and former employees warn about AI risks", "body": "June 4 (Reuters) - A group of current and former employees at artificial intelligence (AI) companies, including Microsoft-backed (MSFT.O)New Tab, opens new tab OpenAI and Alphabet's (GOOGL.O)New Tab, opens new tab Google DeepMind on Tuesday raised concerns about risks posed by the emerging technology.\n\nAn open letter by a group of 11 current and former employees of OpenAI and one current and another former employee with Google DeepMind said the financial motives of AI companies hinder effective oversight.\n\n\"We do not believe bespoke structures of corporate governance are sufficient to change this,\" the letter added.\n\nIt further warns of risks from unregulated AI, ranging from the spread of misinformation to the loss of independent AI systems and the deepening of existing inequalities, which could result in \"human extinction.\"\n\nResearchers have found examples of image generators from companies including OpenAI and Microsoft producing photos with voting-related disinformation, despite policies against such content.\n\nAI companies have \"weak obligations\" to share information with the governments about the capabilities and limitations of their systems, the letter said, adding that these firms cannot be relied upon to share that information voluntarily.\n\nThe open letter is the latest to raise safety concerns around generative AI technology, which can quickly and cheaply produce human-like text, imagery and audio.\n\nThe group has urged AI firms to facilitate a process for current and former employees to raise risk-related concerns and not enforce confidentiality agreements that prohibit criticism.\n\nSeparately, the Sam Altman-led firm said on Thursday it disrupted five covert influence operations that sought to use its artificial intelligence models for \"deceptive activity\" across the internet.\n\nReporting by Jaspreet Singh in Bengaluru; Editing by Tasim Zahid", "source": {"uri": "reuters.com", "dataType": "news", "title": "Reuters"}, "authors": [{"uri": "reuters@reuters.com", "name": "Reuters", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 5, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Emerging_technologies", "type": "wiki", "score": 3, "label": {"eng": "Emerging technologies"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Google", "type": "org", "score": 3, "label": {"eng": "Google"}}, {"uri": "http://en.wikipedia.org/wiki/Reuters", "type": "wiki", "score": 3, "label": {"eng": "Reuters"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 2, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Disinformation", "type": "wiki", "score": 2, "label": {"eng": "Disinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Microsoft", "type": "org", "score": 2, "label": {"eng": "Microsoft"}}, {"uri": "http://en.wikipedia.org/wiki/Generative_artificial_intelligence", "type": "wiki", "score": 1, "label": {"eng": "Generative artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 1, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Internet", "type": "wiki", "score": 1, "label": {"eng": "Internet"}}, {"uri": "http://en.wikipedia.org/wiki/Bangalore", "type": "loc", "score": 1, "label": {"eng": "Bangalore"}, "location": {"type": "place", "label": {"eng": "Bangalore"}, "country": {"type": "country", "label": {"eng": "India"}}}}], "categories": [{"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 20}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 17}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 25}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 21}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 90}], "image": "https://www.reuters.com/resizer/v2/BGKOW7KYVNPLVA7IYVW36IGCMM.jpg?auth=543ea8a308aa907ed95817b6363b720e39ebdfa13459f675e360c61118b212ee&height=1005&width=1920&quality=80&smart=true", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": [{"amb": false, "imp": true, "date": "2024-06-04", "textStart": 0, "textEnd": 6}], "sentiment": -0.05882352941176472, "wgt": 163, "relevance": 1}
{"uri": "8161463840", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "16:24:29", "dateTime": "2024-06-04T16:24:29Z", "dateTimePub": "2024-06-04T16:23:52Z", "dataType": "news", "sim": 0.615686297416687, "url": "https://www.computing.co.uk/news/4268029/staff-ai-companies-sign-letter-calling-transparency-about-risks", "title": "Staff at top AI companies sign letter calling for more transparency about risks", "body": "The signatories say they believe in the potential of AI for solving many of humanity's problems and to deliver unprecedented benefits, but believe that the profit motive is providing \"strong financial incentives to avoid effective oversight,\" adding that the \"bespoke structures of corporate governance\" are not up to the task.\n\nCompanies cannot be relied upon to share risk based information with governments and civil society, they argue. In the absence of proper oversight structures, only former and current employees at AI companies can keep them accountable.\n\nThe signatories note that strict non-disclosure agreements often prevent employees from speaking out, about legal but questionable practices they may have seen.\n\nSeveral reports have detailed how employees at OpenAI have had their stock in the company tied to \"non-disparagement\" agreements.\n\nThe company says it has since reversed this policy, but such practices demonstrates the barriers to transparency and accountability imposed by companies who stand to make vast profits from AI.\n\nOther examples of the opaque nature of operations, are the mysterious hiring and refiring of Sam Altman, CEO at Open AI; the departure of chief scientist and superalignment lead Ilya Sutskever and safety researcher Jan Leike from that company; and the laying off of AI safety teams across the tech sector.\n\nThe open letter, headed \"A Right to Warn about Advanced Artificial Intelligence\", calls on \"advanced AI companies\" to sign up to the following four principles, as confidentiality allows:\n\n1. That the company will not enter into or enforce any agreement that prohibits \"disparagement\" or criticism of the company for risk-related concerns;\n\n2. That the company will facilitate a verifiably anonymous process for current and former employees to raise risk-related concerns to the company's board, to regulators, and to an appropriate independent organisation with relevant expertise;\n\n3. That the company will support a culture of open criticism and allow its current and former employees to raise risk-related concerns about its technologies to the public;\n\n4. That the company will not retaliate against current and former employees who publicly share risk-related confidential information after other processes have failed.\n\nSignatories include Jacob Hilton, formerly a reinforcement learning researcher at OpenAI, Ramana Kumar, a former AGI safety researcher at Google DeepMind, and Neel Nanda, a research engineer at DeepMind who previously worked for Anthropic.\n\nThe letter is endorsed by three of the biggest names in AI development: Yoshua Bengio, Geoffrey Hinton and Stuart Russell.", "source": {"uri": "computing.co.uk", "dataType": "news", "title": "Computing"}, "authors": [{"uri": "john_leonard@computing.co.uk", "name": "John Leonard", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 3, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 2, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Ilya_Sutskever", "type": "person", "score": 2, "label": {"eng": "Ilya Sutskever"}}, {"uri": "http://en.wikipedia.org/wiki/Sam_Altman", "type": "person", "score": 2, "label": {"eng": "Sam Altman"}}, {"uri": "http://en.wikipedia.org/wiki/Transparency_(behavior)", "type": "wiki", "score": 2, "label": {"eng": "Transparency (behavior)"}}, {"uri": "http://en.wikipedia.org/wiki/Chief_executive_officer", "type": "wiki", "score": 2, "label": {"eng": "Chief executive officer"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 1, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_general_intelligence", "type": "wiki", "score": 1, "label": {"eng": "Artificial general intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Yoshua_Bengio", "type": "person", "score": 1, "label": {"eng": "Yoshua Bengio"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 1, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Stuart_J._Russell", "type": "person", "score": 1, "label": {"eng": "Stuart J. Russell"}}, {"uri": "http://en.wikipedia.org/wiki/Geoffrey_Hinton", "type": "person", "score": 1, "label": {"eng": "Geoffrey Hinton"}}, {"uri": "http://en.wikipedia.org/wiki/Reinforcement_learning", "type": "wiki", "score": 1, "label": {"eng": "Reinforcement learning"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 20}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 24}, {"uri": "dmoz/Business/Human_Resources/Training_and_Safety", "label": "dmoz/Business/Human Resources/Training and Safety", "wgt": 19}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 20}], "image": "https://image.chitra.live/api/v1/wps/19a311b/d0991b39-39d5-4646-ab0b-d2e2bcad8b85/4/Shutterstock-loudhailer-2130751166-370x229.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.2784313725490195, "wgt": 157, "relevance": 1}
{"uri": "8161537578", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:20:20", "dateTime": "2024-06-04T17:20:20Z", "dateTimePub": "2024-06-04T17:19:04Z", "dataType": "news", "sim": 0.6039215922355652, "url": "https://timesofindia.indiatimes.com/technology/tech-news/read-letter-by-current-and-former-chatgpt-maker-openai-employees-warning-about-advanced-ai/articleshow/110711942.cms", "title": "Read: Letter by current and former ChatGPT-maker OpenAI employees 'warning' about advanced AI - Times of India", "body": "A group of current and former employees at ChatGPT-maker OpenAI have written a letter citing concerns about the potential risks posed by these technologies. They acknowledge the potential benefits of AI but warn of dangers like increased inequality, manipulation, and loss of control over AI systems.\n\nThis group of employees believe AI companies have a financial incentive to avoid oversight and don't share enough information about the risks with the public.Furthermore, they argue that current protections to whistleblowers are inadequate and confidentiality agreements prevent them from speaking out.\n\nThey also propose principles for AI companies to adopt, including allowing employees to raise concerns anonymously and publicly without fear of retaliation.\n\nHere's the letter:\n\nWe are current and former employees at frontier AI companies, and we believe in the potential of AI technology to deliver unprecedented benefits to humanity.\n\nWe also understand the serious risks posed by these technologies. These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction. AI companies themselves have acknowledged these risks [1, 2, 3], as have governments across the world [4, 5, 6] and other AI experts [7, 8, 9].\n\nWe are hopeful that these risks can be adequately mitigated with sufficient guidance from the scientific community, policymakers, and the public. However, AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this.\n\nAI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm. However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily.\n\nSo long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public. Yet broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues. Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated. Some of us reasonably fear various forms of retaliation, given the history of such cases across the industry. We are not the first to encounter or speak about these issues.\n\nWe therefore call upon advanced AI companies to commit to these principles:\n\nThat the company will not enter into or enforce any agreement that prohibits \"disparagement\" or criticism of the company for risk-related concerns, nor retaliate for risk-related criticism by hindering any vested economic benefit;That the company will facilitate a verifiably anonymous process for current and former employees to raise risk-related concerns to the company's board, to regulators, and to an appropriate independent organization with relevant expertise;That the company will support a culture of open criticism and allow its current and former employees to raise risk-related concerns about its technologies to the public, to the company's board, to regulators, or to an appropriate independent organization with relevant expertise, so long as trade secrets and other intellectual property interests are appropriately protected;That the company will not retaliate against current and former employees who publicly share risk-related confidential information after other processes have failed. We accept that any effort to report risk-related concerns should avoid releasing confidential information unnecessarily. Therefore, once an adequate process for anonymously raising concerns to the company's board, to regulators, and to an appropriate independent organization with relevant expertise exists, we accept that concerns should be raised through such a process initially. However, as long as such a process does not exist, current and former employees should retain their freedom to report their concerns to the public.\n\nThe TOI Tech Desk is a dedicated team of journalists committed to delivering the latest and most relevant news from the world of technology to readers of The Times of India. TOI Tech Desk's news coverage spans a wide spectrum across gadget launches, gadget reviews, trends, in-depth analysis, exclusive reports and breaking stories that impact technology and the digital universe. Be it how-tos or the latest happenings in AI, cybersecurity, personal gadgets, platforms like WhatsApp, Instagram, Facebook and more; TOI Tech Desk brings the news with accuracy and authenticity.", "source": {"uri": "timesofindia.indiatimes.com", "dataType": "news", "title": "The Times of India"}, "authors": [{"uri": "toi_tech_desk@timesofindia.indiatimes.com", "name": "Toi Tech Desk", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 4, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 4, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 3, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 3, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 3, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Economic_inequality", "type": "wiki", "score": 3, "label": {"eng": "Economic inequality"}}, {"uri": "http://en.wikipedia.org/wiki/Corporate_governance", "type": "wiki", "score": 2, "label": {"eng": "Corporate governance"}}, {"uri": "http://en.wikipedia.org/wiki/Voice_acting", "type": "wiki", "score": 2, "label": {"eng": "Voice acting"}}, {"uri": "http://en.wikipedia.org/wiki/Civil_society", "type": "wiki", "score": 2, "label": {"eng": "Civil society"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 2, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/Electromagnetic_spectrum", "type": "wiki", "score": 1, "label": {"eng": "Electromagnetic spectrum"}}, {"uri": "http://en.wikipedia.org/wiki/Trade_secret", "type": "wiki", "score": 1, "label": {"eng": "Trade secret"}}, {"uri": "http://en.wikipedia.org/wiki/The_Times_of_India", "type": "wiki", "score": 1, "label": {"eng": "The Times of India"}}, {"uri": "http://en.wikipedia.org/wiki/Intellectual_property", "type": "wiki", "score": 1, "label": {"eng": "Intellectual property"}}, {"uri": "http://en.wikipedia.org/wiki/Confidentiality", "type": "wiki", "score": 1, "label": {"eng": "Confidentiality"}}, {"uri": "http://en.wikipedia.org/wiki/WhatsApp", "type": "org", "score": 1, "label": {"eng": "WhatsApp"}}, {"uri": "http://en.wikipedia.org/wiki/Computer_security", "type": "wiki", "score": 1, "label": {"eng": "Computer security"}}, {"uri": "http://en.wikipedia.org/wiki/Instagram", "type": "org", "score": 1, "label": {"eng": "Instagram"}}, {"uri": "http://en.wikipedia.org/wiki/Universe", "type": "wiki", "score": 1, "label": {"eng": "Universe"}}, {"uri": "http://en.wikipedia.org/wiki/Facebook", "type": "org", "score": 1, "label": {"eng": "Facebook"}}], "categories": [{"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 19}, {"uri": "dmoz/Business/Human_Resources/Compensation_and_Benefits", "label": "dmoz/Business/Human Resources/Compensation and Benefits", "wgt": 19}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 21}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 23}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 18}, {"uri": "news/Business", "label": "news/Business", "wgt": 70}], "image": "https://static.toiimg.com/thumb/msid-110711930,width-1070,height-580,imgsize-22700,resizemode-75,overlay-toi_sw,pt-32,y_pad-40/photo.jpg", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.1764705882352942, "wgt": 154, "relevance": 1}
{"uri": "8161368438", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "15:21:29", "dateTime": "2024-06-04T15:21:29Z", "dateTimePub": "2024-06-04T15:19:00Z", "dataType": "news", "sim": 0.5921568870544434, "url": "https://news.bloomberglaw.com/artificial-intelligence/openai-employees-call-for-protections-to-speak-out-on-ai-risks", "title": "OpenAI Employees Call For Protections to Speak Out on AI Risks", "body": "A group of current and former employees from OpenAI and Google DeepMind have signed an open letter asking for protection from retaliation for sharing concerns about the \"serious risks\" of the technologies these and other companies are building.\n\n\"So long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public,\" according to the letter, which was signed by 13 people who've worked at the companies, seven of whom included their names. \"Yet broad confidentiality agreements block us from voicing our concerns, except to the ...", "source": {"uri": "news.bloomberglaw.com", "dataType": "news", "title": "news.bloomberglaw.com"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 3, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 3, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 2, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 1, "label": {"eng": "Non-disclosure agreement"}}], "categories": [{"uri": "dmoz/Society/Work", "label": "dmoz/Society/Work", "wgt": 18}, {"uri": "dmoz/Business/Business_Services/Signage", "label": "dmoz/Business/Business Services/Signage", "wgt": 25}, {"uri": "dmoz/Society/Work/Coworker_Relations", "label": "dmoz/Society/Work/Coworker Relations", "wgt": 21}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 27}, {"uri": "dmoz/Recreation/Roads_and_Highways/Signs_and_Signals", "label": "dmoz/Recreation/Roads and Highways/Signs and Signals", "wgt": 19}], "image": "https://news-cdn.bindg.com/indg/assets/news/images/Fallback-image.webp", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.04313725490196085, "wgt": 151, "relevance": 1}
{"uri": "8161269217", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "14:15:32", "dateTime": "2024-06-04T14:15:32Z", "dateTimePub": "2024-06-04T14:14:23Z", "dataType": "news", "sim": 0.572549045085907, "url": "https://www.washingtonpost.com/technology/2024/06/04/openai-employees-ai-whistleblowers/", "title": "Current and former AI employees warn of the technology's dangers", "body": "The employees said that absent government oversight, AI workers are the \"few people\" that can hold corporations accountable. But they noted that they are hamstrung by \"broad confidentiality agreements\" and that ordinary whistleblower protections are \"insufficient\" because they focus on illegal activity, and the risks that they are warning about are not yet regulated.\n\nThe letter called for AI companies to commit to four principles to allow for greater transparency and whistleblower protections. Those principles included a commitment to not enter into or enforce agreements that prohibit criticism of risks; a call to establish an anonymous process for current and former employees to raise concerns; supporting a culture of criticism; and a promise to not retaliate against current and former employees who share confidential information to raise alarms \"after other processes have failed.\"", "source": {"uri": "washingtonpost.com", "dataType": "news", "title": "Washington Post"}, "authors": [{"uri": "pranshu_verma@washingtonpost.com", "name": "Pranshu Verma", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 4, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/Non-disclosure_agreement", "type": "wiki", "score": 3, "label": {"eng": "Non-disclosure agreement"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 3, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Corporation", "type": "wiki", "score": 3, "label": {"eng": "Corporation"}}, {"uri": "http://en.wikipedia.org/wiki/Transparency_(behavior)", "type": "wiki", "score": 2, "label": {"eng": "Transparency (behavior)"}}], "categories": [{"uri": "dmoz/Society/Work", "label": "dmoz/Society/Work", "wgt": 15}, {"uri": "dmoz/Business/Human_Resources", "label": "dmoz/Business/Human Resources", "wgt": 14}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 29}, {"uri": "dmoz/Society/Work/Coworker_Relations", "label": "dmoz/Society/Work/Coworker Relations", "wgt": 15}, {"uri": "dmoz/Business/Human_Resources/Employee_Relations", "label": "dmoz/Business/Human Resources/Employee Relations", "wgt": 21}], "image": "https://www.washingtonpost.com/wp-apps/imrs.php?src=https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/4CVDCEZVYSOG5DKYDV57L355RI_size-normalized.jpg&w=1440", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.05882352941176472, "wgt": 146, "relevance": 1}
{"uri": "8161540696", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:22:06", "dateTime": "2024-06-04T17:22:06Z", "dateTimePub": "2024-06-04T17:21:30Z", "dataType": "news", "sim": 0.5647059082984924, "url": "https://www.annistonstar.com/news/nation_world/openai-insiders-blast-lack-of-ai-transparency/article_53cbf1f8-d2a4-52aa-b6fe-3056e618526b.html", "title": "OpenAI insiders blast lack of AI transparency", "body": "A group of current and former employees from OpenAI on Tuesday issued an open letter warning that the world's leading artificial intelligence companies were falling short of necessary transparency and accountability to meet the potential risks posed by the technology.\n\nThe letter raised serious concerns about AI safety risks \"ranging from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction.\"", "source": {"uri": "annistonstar.com", "dataType": "news", "title": "The Anniston Star"}, "authors": [{"uri": "agence_france_presse@annistonstar.com", "name": "Agence France-Presse", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 3, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 2, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 1, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 1, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 1, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Trench_warfare", "type": "wiki", "score": 1, "label": {"eng": "Trench warfare"}}], "categories": [{"uri": "dmoz/Computers/Artificial_Intelligence/Associations", "label": "dmoz/Computers/Artificial Intelligence/Associations", "wgt": 15}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 23}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 16}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 22}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 18}], "image": "https://bloximages.chicago2.vip.townnews.com/annistonstar.com/content/tncms/assets/v3/editorial/8/3d/83debaa0-26a2-5000-880f-d34c69456161/664d1e2aa3e1e.image.jpg?crop=512%2C269%2C0%2C36&resize=438%2C230&order=crop%2Cresize", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": -0.4509803921568627, "wgt": 144, "relevance": 1}
{"uri": "8161524375", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:10:51", "dateTime": "2024-06-04T17:10:51Z", "dateTimePub": "2024-06-04T17:09:15Z", "dataType": "news", "sim": 0.5411764979362488, "url": "https://www.rocketnews.com/2024/06/more-openai-researchers-slam-company-on-safety-call-for-right-to-warn-to-avert-human-extinction/", "title": "More OpenAI researchers slam company on safety, call for 'right to warn' to avert 'human extinction' - RocketNews", "body": "Time's almost up! There's only one week left to request an invite to The AI Impact Tour on June 5th. Don't miss out on this incredible opportunity to explore various methods for auditing AI models. Find out how you can attend here.\n\nA group of 11 researchers who currently or formerly worked at OpenAI, as well as a current member of Google DeepMind who previously worked at Anthropic, and another former DeepMind researcher, have signed a new open letter online calling for OpenAI and similar companies to commit to four principles protecting whistleblowers and critics who raise issues surrounding AI safety.\n\n\"We also understand the serious risks posed by these technologies,\" the letter, titled \"Right to Warn,\" states: \"These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction.\"\n\nWhat is a 'Right to Warn' for AI systems?\n\nAmong the concerns expressed in the letter are the lack of proper oversight, the influence of profit motives, and the suppression of dissenting voices within organizations working on cutting-edge AI technologies.\n\nThe four principles the signatories want AI companies to voluntarily agree to abide by to rectify these are as follows:\n\nJune 5th: The AI Audit in NYC\n\nJoin us next week in NYC to engage with top executive leaders, delving into strategies for auditing AI models to ensure optimal performance and accuracy across your organization. Secure your attendance for this exclusive invite-only event.\n\nRefraining from enforcing agreements that prohibit disparaging comments or retaliation for risk-related criticism\n\nEstablishing a verifiable anonymous process for raising risk-related concerns to the company's board, regulators, and independent organizations\n\nEncouraging a c ...", "source": {"uri": "rocketnews.com", "dataType": "news", "title": "RocketNews | Top News Stories From Around the Globe"}, "authors": [], "concepts": [{"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}, {"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 4, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/DeepMind", "type": "org", "score": 4, "label": {"eng": "DeepMind"}}, {"uri": "http://en.wikipedia.org/wiki/Anthropic", "type": "wiki", "score": 3, "label": {"eng": "Anthropic"}}, {"uri": "http://en.wikipedia.org/wiki/Open_letter", "type": "wiki", "score": 3, "label": {"eng": "Open letter"}}, {"uri": "http://en.wikipedia.org/wiki/AI_safety", "type": "wiki", "score": 2, "label": {"eng": "AI safety"}}, {"uri": "http://en.wikipedia.org/wiki/Misinformation", "type": "wiki", "score": 2, "label": {"eng": "Misinformation"}}, {"uri": "http://en.wikipedia.org/wiki/Human_extinction", "type": "wiki", "score": 2, "label": {"eng": "Human extinction"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 2, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/New_York_City", "type": "loc", "score": 1, "label": {"eng": "New York City"}, "location": {"type": "place", "label": {"eng": "New York City"}, "country": {"type": "country", "label": {"eng": "United States"}}}}], "categories": [{"uri": "dmoz/Computers/Artificial_Intelligence", "label": "dmoz/Computers/Artificial Intelligence", "wgt": 14}, {"uri": "dmoz/Society/Future/Catastrophes", "label": "dmoz/Society/Future/Catastrophes", "wgt": 16}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 18}, {"uri": "dmoz/Computers/Artificial_Intelligence/Games", "label": "dmoz/Computers/Artificial Intelligence/Games", "wgt": 24}, {"uri": "dmoz/Computers/Artificial_Intelligence/Philosophy", "label": "dmoz/Computers/Artificial Intelligence/Philosophy", "wgt": 19}, {"uri": "news/Technology", "label": "news/Technology", "wgt": 54}], "image": null, "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": [{"amb": false, "imp": true, "date": "2024-06-05", "textStart": 91, "textEnd": 99, "freq": 2}], "sentiment": 0.1529411764705881, "wgt": 138, "relevance": 1}
{"uri": "2024-06-378709206", "lang": "eng", "isDuplicate": false, "date": "2024-06-04", "time": "17:19:24", "dateTime": "2024-06-04T17:19:24Z", "dateTimePub": "2024-06-04T17:00:00Z", "dataType": "news", "sim": 0.43529412150383, "url": "https://www.bakersfield.com/ap/news/former-openai-employees-lead-push-to-protect-whistleblowers-flagging-artificial-intelligence-risks/article_a6c7fa4c-abd5-5ba3-b705-829011aa22a0.html", "title": "Former OpenAI employees lead push to protect whistleblowers flagging artificial intelligence risks", "body": "A group of OpenAI's current and former workers are calling on the ChatGPT-maker and other artificial intelligence companies to protect whistleblowing employees who flag safety risks about AI technology\n\nA group of OpenAI's current and former workers is calling on the ChatGPT-maker and other artificial intelligence companies to protect whistleblowing employees who flag safety risks about AI technology.\n\nAn open letter published Tuesday asks tech companies to establish stronger whistleblower protections so researchers can raise concerns about the development of high-performing AI systems internally and with the public without fear of retaliation.", "source": {"uri": "bakersfield.com", "dataType": "news", "title": "The Bakersfield Californian"}, "authors": [{"uri": "matt_o_brien@bakersfield.com", "name": "Matt O'Brien", "type": "author", "isAgency": false}], "concepts": [{"uri": "http://en.wikipedia.org/wiki/OpenAI", "type": "wiki", "score": 5, "label": {"eng": "OpenAI"}}, {"uri": "http://en.wikipedia.org/wiki/Whistleblower", "type": "wiki", "score": 5, "label": {"eng": "Whistleblower"}}, {"uri": "http://en.wikipedia.org/wiki/Artificial_intelligence", "type": "wiki", "score": 5, "label": {"eng": "Artificial intelligence"}}], "categories": [{"uri": "dmoz/Shopping/Home_and_Garden/Flags_and_Banners", "label": "dmoz/Shopping/Home and Garden/Flags and Banners", "wgt": 24}, {"uri": "dmoz/Society/Work/Whistleblowing", "label": "dmoz/Society/Work/Whistleblowing", "wgt": 45}, {"uri": "dmoz/Computers/Artificial_Intelligence/Associations", "label": "dmoz/Computers/Artificial Intelligence/Associations", "wgt": 27}, {"uri": "dmoz/Computers/Artificial_Intelligence/Publications", "label": "dmoz/Computers/Artificial Intelligence/Publications", "wgt": 31}, {"uri": "dmoz/Computers/Artificial_Intelligence/Academic_Departments", "label": "dmoz/Computers/Artificial Intelligence/Academic Departments", "wgt": 27}], "image": "https://bloximages.newyork1.vip.townnews.com/bakersfield.com/content/tncms/assets/v3/editorial/a/f5/af56d215-449d-56d7-971d-c66d1a901170/665f49d45750a.image.jpg?crop=1763%2C926%2C0%2C124&resize=1200%2C630&order=crop%2Cresize", "originalArticle": null, "storyUri": "eng-9624036", "eventUri": "eng-9624036", "location": null, "extractedDates": null, "sentiment": 0.7019607843137254, "wgt": 111, "relevance": 1}
